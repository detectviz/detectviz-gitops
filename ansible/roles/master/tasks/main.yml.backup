---
- name: Check if kubeadm has been initialized
  ansible.builtin.stat:
    path: /etc/kubernetes/admin.conf
  register: kubeadm_initialized

- name: Ensure Kubernetes directories exist
  ansible.builtin.file:
    path: "{{ item.path }}"
    state: directory
    mode: "{{ item.mode }}"
    owner: "{{ item.owner | default('root') }}"
    group: "{{ item.group | default('root') }}"
  loop:
    - { path: "/etc/kubernetes", mode: "0755" }
    - { path: "/etc/kubernetes/manifests", mode: "0755" }
    - { path: "/var/lib/kubelet", mode: "0750" }
    - { path: "/var/lib/etcd", mode: "0700" }
  become: true
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: "[P2] Install Kube-VIP required packages"
  ansible.builtin.apt:
    name: ["arping", "jq"]
    state: present
  become: true

- name: "[P2] Download Kube-VIP Manifest"
  ansible.builtin.get_url:
    url: "https://raw.githubusercontent.com/kube-vip/kube-vip-cloud-provider/main/manifest/kube-vip-cloud-controller.yaml"
    dest: "/etc/kubernetes/manifests/kube-vip-cloud-controller.yaml"
    mode: "0644"
  become: true
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: "[P2] Create Kube-VIP DaemonSet Manifest for Control Plane HA"
  ansible.builtin.template:
    src: kube-vip-ds.yaml.j2
    dest: "/etc/kubernetes/manifests/kube-vip-ds.yaml"
    mode: "0644"
  become: true
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Clean up any previous Kubernetes installation
  ansible.builtin.command: kubeadm reset --force --cri-socket unix:///var/run/containerd/containerd.sock
  ignore_errors: true
  become: true
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Remove old Kubernetes manifests and configs
  ansible.builtin.file:
    path: "{{ item }}"
    state: absent
  loop:
    - /etc/kubernetes
    - /var/lib/kubelet
    - /var/lib/etcd
  ignore_errors: true
  become: true
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Restart containerd to apply new sandbox image
  ansible.builtin.systemd:
    name: containerd
    state: restarted
    daemon_reload: true
  become: true
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Verify containerd configuration
  ansible.builtin.command: containerd --version && crictl version
  become: true
  when: inventory_hostname == groups['masters'][0']

- name: Check containerd sandbox image
  ansible.builtin.command: grep sandbox_image /etc/containerd/config.toml
  become: true
  register: sandbox_check
  when: inventory_hostname == groups['masters'][0']

- name: Display containerd sandbox image setting
  ansible.builtin.debug:
    msg: "Containerd sandbox image: {{ sandbox_check.stdout | default('not found') }}"
  when:
    - inventory_hostname == groups['masters'][0]
    - sandbox_check is defined

- name: Generate kubeadm config
  ansible.builtin.template:
    src: kubeadm-config.yaml.j2
    dest: /tmp/kubeadm-config.yaml
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Pre-pull Kubernetes images
  ansible.builtin.command: kubeadm config images pull
  become: true
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Initialize first control plane node
  ansible.builtin.command: >
    timeout 600 kubeadm init --config /tmp/kubeadm-config.yaml
    --ignore-preflight-errors=all --skip-phases=addon/kube-proxy
    --v=3
  args:
    creates: /etc/kubernetes/admin.conf
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Ensure .kube directory exists for root user
  ansible.builtin.file:
    path: /root/.kube
    state: directory
    mode: "0700"
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Copy admin kubeconfig to root's .kube directory
  ansible.builtin.copy:
    src: /etc/kubernetes/admin.conf
    dest: /root/.kube/config
    remote_src: yes
    mode: "0600"
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Fetch admin kubeconfig to control node
  ansible.builtin.fetch:
    src: /etc/kubernetes/admin.conf
    dest: "{{ playbook_dir }}/kubeconfig/admin.conf"
    flat: yes
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Wait for API server to be ready
  ansible.builtin.pause:
    seconds: 30
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Check DNS resolution for API server
  ansible.builtin.shell: |
    if nslookup k8s-api.detectviz.internal >/dev/null 2>&1; then
      echo "DNS resolution successful"
    elif host k8s-api.detectviz.internal >/dev/null 2>&1; then
      echo "DNS resolution successful (host command)"
    else
      echo "DNS lookup failed"
    fi
  args:
    executable: /bin/bash
  become: true
  register: dns_check
  changed_when: false
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Display DNS check result
  ansible.builtin.debug:
    msg: "DNS resolution result: {{ dns_check.stdout }}"
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Add API server to /etc/hosts as fallback
  ansible.builtin.lineinfile:
    path: /etc/hosts
    line: "{{ control_plane_vip }} k8s-api.detectviz.internal k8s-api"
    state: present
  become: true
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Update kubeconfig server URLs to use local IP
  ansible.builtin.lineinfile:
    path: "{{ item }}"
    regexp: "server: https://k8s-api.detectviz.internal:6443"
    line: "server: https://{{ ansible_default_ipv4.address }}:6443"
  loop:
    - /etc/kubernetes/admin.conf
    - /etc/kubernetes/super-admin.conf
  become: true
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Check API server health
  ansible.builtin.command: kubectl --kubeconfig=/etc/kubernetes/admin.conf get nodes
  register: api_health_check
  retries: 5
  delay: 5
  until: api_health_check.rc == 0
  changed_when: false
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Create cluster-admin clusterrolebinding for kubernetes-admin user
  ansible.builtin.command: >
    kubectl --kubeconfig=/etc/kubernetes/super-admin.conf create clusterrolebinding kubernetes-admin-binding
    --clusterrole=cluster-admin --user=kubernetes-admin
  ignore_errors: true
  changed_when: false
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Upload control plane certificates
  ansible.builtin.command: kubeadm init phase upload-certs --upload-certs
  register: kubeadm_upload_certs
  changed_when: false
  retries: 5
  delay: 15
  until: kubeadm_upload_certs.rc == 0
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Set kubeadm certificate key fact
  ansible.builtin.set_fact:
    kubeadm_certificate_key: "{{ kubeadm_upload_certs.stdout | regex_search('[a-f0-9]{64}') }}"
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Create kubeadm join command token
  ansible.builtin.shell: |
    # 使用 kubectl 創建 token 和 join 命令
    TOKEN=$(kubectl --kubeconfig=/etc/kubernetes/admin.conf create token --ttl=24h)
    CA_CERT_HASH=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //')
    echo "kubeadm join {{ control_plane_endpoint }} --token $TOKEN --discovery-token-ca-cert-hash sha256:$CA_CERT_HASH"
  register: kubeadm_join_command
  changed_when: false
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Set join command facts on master
  ansible.builtin.set_fact:
    control_plane_join_command: "{{ kubeadm_join_command.stdout }} --control-plane{{ ' --certificate-key ' + kubeadm_certificate_key if kubeadm_certificate_key is defined else '' }}"
    worker_join_command: "{{ kubeadm_join_command.stdout }}"
  when:
    - "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"
    - kubeadm_join_command is defined

- name: "[P2] Download Calico Manifest"
  ansible.builtin.get_url:
    url: "{{ calico_manifest_url }}"
    dest: "{{ calico_manifest_local_path }}"
    mode: "0644"
  become: false
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: "[P2] Modify Calico Manifest with Pod CIDR"
  ansible.builtin.lineinfile:
    path: "{{ calico_manifest_local_path }}"
    regexp: '^(.*"name": "CALICO_IPV4POOL_CIDR", "value": ").*(")$'
    line: '\1{{ pod_network_cidr }}\2'
    backrefs: true
  become: false
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: "[P2] Apply Calico CNI"
  ansible.builtin.command: "kubectl --kubeconfig=/etc/kubernetes/admin.conf apply -f {{ calico_manifest_local_path }} --validate=false"
  become: true
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"
  changed_when: true

- name: "[P2] Skip kube-proxy configuration (will use defaults)"
  ansible.builtin.debug:
    msg: "Skipping custom kube-proxy configuration - using Kubernetes defaults"
  when: "'masters' in group_names and groups['masters'].index(inventory_hostname) == 0"

- name: Join additional control plane nodes
  ansible.builtin.command: "{{ hostvars[groups['masters'][0]]['control_plane_join_command'] }}"
  args:
    creates: /etc/kubernetes/kubelet.conf
  when:
    - inventory_hostname != groups['masters'][0]
