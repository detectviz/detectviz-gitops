# DetectViz GitOps å¸¸è¦‹å•é¡Œ (FAQ)

åŸºæ–¼å¯¦éš›éƒ¨ç½²æ—¥èªŒï¼ˆdeploy.logï¼‰æ•´ç†çš„å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆã€‚

## ç›®éŒ„

- [é›ç”Ÿè›‹å•é¡Œ](#é›ç”Ÿè›‹å•é¡Œ)
- [ArgoCD ç›¸é—œå•é¡Œ](#argocd-ç›¸é—œå•é¡Œ)
- [TopoLVM å­˜å„²å•é¡Œ](#topolvm-å­˜å„²å•é¡Œ)
- [ç¶²è·¯èˆ‡ DNS å•é¡Œ](#ç¶²è·¯èˆ‡-dns-å•é¡Œ)
- [Ansible éƒ¨ç½²å•é¡Œ](#ansible-éƒ¨ç½²å•é¡Œ)
- [Vault ç›¸é—œå•é¡Œ](#vault-ç›¸é—œå•é¡Œ)

---


## é›ç”Ÿè›‹å•é¡Œ

æœ¬éƒ¨ç½²æµç¨‹å·²å®Œæ•´è§£æ±ºä»¥ä¸‹å¾ªç’°ä¾è³´å•é¡Œï¼ˆè©³è¦‹[æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)ç« ç¯€ï¼‰ï¼š

### å•é¡Œ #1: ApplicationSet è·¯å¾‘é…ç½®
- **ç—‡ç‹€**: ArgoCD ç„¡æ³•æ‰¾åˆ°æ‡‰ç”¨è·¯å¾‘
- **è§£æ±ºæ–¹æ¡ˆ**: âœ… æ‰€æœ‰ ApplicationSet è·¯å¾‘å·²åŒ…å« `argocd/` å‰ç¶´
- **é©—è­‰**: `argocd/appsets/appset.yaml` å·²ä¿®æ­£

### å•é¡Œ #2: AppProject æ¬Šé™ç™½åå–®
- **ç—‡ç‹€**: åŸºç¤è¨­æ–½æ‡‰ç”¨ç„¡æ³•å‰µå»º Namespace æˆ– IngressClass
- **è§£æ±ºæ–¹æ¡ˆ**: âœ… `platform-bootstrap` é …ç›®å·²åŒ…å«æ‰€æœ‰å¿…è¦è³‡æºæ¬Šé™
- **é©—è­‰**: `argocd/bootstrap/argocd-projects.yaml` å·²é…ç½®å®Œæ•´

### å•é¡Œ #3: CRD ä¾è³´é †åº
- **ç—‡ç‹€**: cluster-bootstrap å˜—è©¦å‰µå»º Certificate ä½† cert-manager CRD å°šæœªå®‰è£
- **è§£æ±ºæ–¹æ¡ˆ**: âœ… ä½¿ç”¨ Sync Wave åˆ†éšæ®µéƒ¨ç½² + `SkipDryRunOnMissingResource=true`
- **é æœŸè¡Œç‚º**: cluster-bootstrap Phase 2 æœƒå…ˆå¤±æ•—ï¼Œå¾…åŸºç¤è¨­æ–½åŒæ­¥å¾Œè‡ªå‹•é‡è©¦æˆåŠŸ
- **é©—è­‰**: åŸºç¤è¨­æ–½åŒæ­¥å¾Œ cluster-bootstrap è‡ªå‹•è®Šç‚º Synced

### å•é¡Œ #4: TopoLVM èª¿åº¦æ¨¡å¼
- **ç—‡ç‹€**: Vault pods é¡¯ç¤º "Insufficient capacity" ä½†å¯¦éš›æœ‰è¶³å¤ ç©ºé–“
- **æ ¹æœ¬åŸå› **: Scheduler Extender æ¨¡å¼æœªå®Œæ•´é…ç½®
- **è§£æ±ºæ–¹æ¡ˆ**: âœ… æ”¹ç”¨ Storage Capacity Tracking æ¨¡å¼ï¼ˆKubernetes 1.21+ åŸç”Ÿï¼‰
- **é©—è­‰**: `argocd/apps/infrastructure/topolvm/overlays/values.yaml` å·²å•Ÿç”¨ `storageCapacityTracking`

### å•é¡Œ #5: Vault Pod Anti-Affinity èˆ‡å–® Worker Node
- **ç—‡ç‹€**: vault-1/vault-2 pods æŒçºŒ Pendingï¼ŒéŒ¯èª¤ "didn't match pod anti-affinity rules"
- **æ ¹æœ¬åŸå› **: Vault Helm chart é»˜èªä½¿ç”¨ `requiredDuringSchedulingIgnoredDuringExecution` anti-affinityï¼Œè¦æ±‚æ¯å€‹ pod åœ¨ä¸åŒ node ä¸Šï¼Œä½†æ¸¬è©¦ç’°å¢ƒåªæœ‰ 1 å€‹ worker node
- **è§£æ±ºæ–¹æ¡ˆ**: âœ… æ”¹ç”¨ `preferredDuringSchedulingIgnoredDuringExecution` (weight: 100)
  - å…è¨±å¤šå€‹ Vault pods åœ¨åŒä¸€ node ä¸Šé‹è¡Œï¼ˆæ¸¬è©¦ç’°å¢ƒï¼‰
  - ç•¶æœ‰å¤šå€‹ worker nodes æ™‚ä»æœƒå˜—è©¦åˆ†æ•£ï¼ˆç”Ÿç”¢ç’°å¢ƒï¼‰
- **é©—è­‰**: `argocd/apps/infrastructure/vault/overlays/values.yaml` å·²æ·»åŠ  `server.affinity` é…ç½®
- **ç”Ÿç”¢å»ºè­°**: å¤š worker node ç’°å¢ƒå¯è€ƒæ…®æ”¹å› `required` ä»¥æé«˜å¯ç”¨æ€§

### å•é¡Œ #6: ArgoCD Server URL é…ç½®æœªç”Ÿæ•ˆ
- **ç—‡ç‹€**: ArgoCD UI ç„¡æ³•æ­£ç¢ºé¡¯ç¤º `https://argocd.detectviz.internal` URL,å½±éŸ¿ SSO å›èª¿å’Œç‹€æ…‹å¾½ç« 
- **æ ¹æœ¬åŸå› **: ArgoCD ç”± Ansible é€šé Helm chart å®‰è£,`argocd-cm.yaml` é…ç½®å¾æœªè¢«æ‡‰ç”¨åˆ°å¯¦éš›é‹è¡Œçš„ ConfigMap
- **è§£æ±ºæ–¹æ¡ˆ**: âœ… å•Ÿç”¨ ArgoCD è‡ªæˆ‘ç®¡ç†é…ç½®
  - æ·»åŠ  ArgoCD åˆ° ApplicationSet (`argocd/appsets/appset.yaml`)
  - å‰µå»º config-only ç®¡ç†æ¨¡å¼ï¼ˆä¸é‡æ–°éƒ¨ç½² ArgoCD æœ¬èº«ï¼‰
  - åªç®¡ç†é…ç½®æ–‡ä»¶ (`argocd-cm.yaml`)ï¼Œé¿å…èˆ‡ Ansible å®‰è£è¡çª
- **è‡¨æ™‚ä¿®å¾©**: å·²æ‰‹å‹• patch ConfigMap: `kubectl patch configmap argocd-cm -n argocd --type merge -p '{"data":{"url":"https://argocd.detectviz.internal"}}'`
- **é©—è­‰**: `argocd/apps/infrastructure/argocd/overlays/kustomization.yaml` å·²æ”¹ç‚º config-only æ¨¡å¼
- **å½±éŸ¿**: æœªä¾†é…ç½®è®Šæ›´å¯é€šé GitOps ç®¡ç†,ç„¡éœ€æ‰‹å‹•æ“ä½œ

### å•é¡Œ #7: Ingress-Nginx LoadBalancer ç„¡æ³•åˆ†é… IP
- **ç—‡ç‹€**: ingress-nginx-controller æœå‹™ EXTERNAL-IP ç‚º `<pending>`ï¼Œç„¡æ³•è¨ªå• https://argocd.detectviz.internal
- **æ ¹æœ¬åŸå› **:
  1. MetalLB IP æ± é…ç½®ä¸å®Œæ•´ï¼ˆç¼ºå°‘ 192.168.0.10ï¼‰
  2. ä½¿ç”¨ deprecated `spec.loadBalancerIP` æ¬„ä½èˆ‡è¨»è§£è¡çª
  3. `externalTrafficPolicy: Local` å°è‡´å¥åº·æª¢æŸ¥å¤±æ•—ï¼ŒIP è¢«æ’¤å›
- **è§£æ±ºæ–¹æ¡ˆ**: âœ… å®Œæ•´ä¿®å¾©é…ç½®
  - æ·»åŠ  `192.168.0.10/32` åˆ° MetalLB IPAddressPool
  - ç§»é™¤ deprecated `spec.loadBalancerIP` æ¬„ä½
  - ä½¿ç”¨ `externalTrafficPolicy: Cluster` æ¨¡å¼
  - é€šé strategic merge patch æ­£ç¢ºé…ç½®æœå‹™
- **é©—è­‰**: EXTERNAL-IP æˆåŠŸåˆ†é…ç‚º 192.168.0.10ï¼ŒHTTPS æ­£å¸¸è¨ªå•
- **ç›¸é—œæ–‡ä»¶**: `ingress-nginx-loadbalancer-fix.md`
- **Commits**: bbab4f2, 16bb52d, 8bafac7, 959332d

**éƒ¨ç½²å»ºè­°**:
- âš ï¸ **cluster-bootstrap é¡¯ç¤º OutOfSync æ˜¯æ­£å¸¸çš„**ï¼Œåœ¨åŸºç¤è¨­æ–½åŒæ­¥å‰æœƒæŒçºŒæ­¤ç‹€æ…‹
- âœ… **æ‰€æœ‰é…ç½®æ–‡ä»¶å·²ä¿®æ­£**ï¼Œç„¡éœ€æ‰‹å‹•èª¿æ•´
- ğŸ“‹ **éµå¾ªæœ¬æ–‡ä»¶æ­¥é©Ÿ**ï¼Œå•é¡Œæœƒè‡ªå‹•è§£æ±º


## ArgoCD ç›¸é—œå•é¡Œ

### Q1: ArgoCD é¡¯ç¤º "app path does not exist" éŒ¯èª¤

**å®Œæ•´éŒ¯èª¤è¨Šæ¯**:
```
ComparisonError: Failed to load target state: failed to generate manifest for source 1 of 1
rpc error: code = Unknown desc = apps/infrastructure/cert-manager/overlays: app path does not exist
```

**åŸå› **: ApplicationSet ç”Ÿæˆçš„æ‡‰ç”¨è·¯å¾‘ç¼ºå°‘ `argocd/` å‰ç¶´

**è¨ºæ–·æ­¥é©Ÿ**:
```bash
# 1. æª¢æŸ¥ Application çš„å¯¦éš›è·¯å¾‘
kubectl get application infra-cert-manager -n argocd -o jsonpath='{.spec.source.path}'

# éŒ¯èª¤è¼¸å‡º: apps/infrastructure/cert-manager/overlays
# æ­£ç¢ºè¼¸å‡º: argocd/apps/infrastructure/cert-manager/overlays

# 2. æª¢æŸ¥ ApplicationSet é…ç½®
kubectl get applicationset detectviz-gitops -n argocd -o yaml | grep -A 5 "elements:"
```

**è§£æ±ºæ–¹æ¡ˆ**:

1. ä¿®æ­£ `argocd/appsets/appset.yaml`:
```yaml
generators:
  - list:
      elements:
        - appName: cert-manager
          path: argocd/apps/infrastructure/cert-manager/overlays  # âœ… æ·»åŠ  argocd/ å‰ç¶´
        - appName: metallb
          path: argocd/apps/infrastructure/metallb/overlays
        # ... å…¶ä»–æ‡‰ç”¨
```

2. æäº¤ä¿®æ”¹ä¸¦åˆ·æ–° root application:
```bash
git add argocd/appsets/appset.yaml
git commit -m "fix: Add argocd/ prefix to application paths"
git push

# åˆ·æ–° root application
kubectl patch application root -n argocd \
  -p='{"metadata":{"annotations":{"argocd.argoproj.io/refresh":"hard"}}}' --type=merge
```

**é é˜²æªæ–½**: æ‰€æœ‰ ApplicationSet è·¯å¾‘éƒ½æ‡‰ä½¿ç”¨å®Œæ•´è·¯å¾‘ï¼ŒåŒ…å« `argocd/` å‰ç¶´ã€‚

---

### Q2: ArgoCD é¡¯ç¤º "resource is not permitted in project" éŒ¯èª¤

**å®Œæ•´éŒ¯èª¤è¨Šæ¯**:
```
resource :Namespace is not permitted in project platform-bootstrap
resource :IngressClass is not permitted in project platform-bootstrap
```

**åŸå› **: AppProject çš„ `clusterResourceWhitelist` æœªåŒ…å«å¿…è¦çš„è³‡æºé¡å‹

**è¨ºæ–·æ­¥é©Ÿ**:
```bash
# 1. æª¢æŸ¥ Application éŒ¯èª¤è©³æƒ…
kubectl get application infra-cert-manager -n argocd -o yaml | grep -A 20 "conditions:"

# 2. æª¢æŸ¥ AppProject ç™½åå–®
kubectl get appproject platform-bootstrap -n argocd -o yaml | grep -A 30 "clusterResourceWhitelist"
```

**è§£æ±ºæ–¹æ¡ˆ**:

ä¿®æ­£ `argocd/bootstrap/argocd-projects.yaml`:
```yaml
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: platform-bootstrap
spec:
  clusterResourceWhitelist:
    - group: ""
      kind: Namespace          # âœ… æ·»åŠ  Namespace
    - group: networking.k8s.io
      kind: IngressClass       # âœ… æ·»åŠ  IngressClass
    - group: apiextensions.k8s.io
      kind: CustomResourceDefinition
    - group: storage.k8s.io
      kind: StorageClass
    - group: rbac.authorization.k8s.io
      kind: ClusterRole
    - group: rbac.authorization.k8s.io
      kind: ClusterRoleBinding
    # ... å…¶ä»–å¿…è¦è³‡æº
```

**æäº¤ä¿®æ”¹**:
```bash
git add argocd/bootstrap/argocd-projects.yaml
git commit -m "fix: Add missing resources to AppProject whitelist"
git push
```

**å¸¸è¦‹ç¼ºå°‘çš„è³‡æºé¡å‹**:
- `Namespace` (core/v1)
- `IngressClass` (networking.k8s.io/v1)
- `StorageClass` (storage.k8s.io/v1)
- `PriorityClass` (scheduling.k8s.io/v1)

---

### Q3: cluster-bootstrap æŒçºŒé¡¯ç¤º OutOfSync å’Œ "CRDs are not installed" éŒ¯èª¤

**å®Œæ•´éŒ¯èª¤è¨Šæ¯**:
```
cluster-bootstrap: OutOfSync, Progressing
no matches for kind "Certificate" in version "cert-manager.io/v1"
ensure CRDs are installed first
```

**é‡è¦**: **é€™æ˜¯æ­£å¸¸ä¸”é æœŸçš„è¡Œç‚ºï¼**

**åŸå› **: cluster-bootstrap Phase 2 è³‡æºï¼ˆCertificates, Ingress, IngressClassï¼‰ä¾è³´ cert-manager å’Œ ingress-nginx çš„ CRDsï¼Œä½†é€™äº›åŸºç¤è¨­æ–½å°šæœªéƒ¨ç½²ã€‚

**è¨­è¨ˆåŸç†**: ä½¿ç”¨ Sync Wave åˆ†éšæ®µéƒ¨ç½²
- **Phase 1** (Sync Wave: -10): Namespaces â†’ ç«‹å³æˆåŠŸ âœ…
- **Phase 2** (Sync Wave: 10): Certificates, Ingress â†’ ç­‰å¾… CRDs â³

**è§£æ±ºæ­¥é©Ÿ**:

1. **ç¢ºèªé€™æ˜¯é æœŸè¡Œç‚º** - åœ¨åŸºç¤è¨­æ–½åŒæ­¥å‰çœ‹åˆ°æ­¤éŒ¯èª¤æ˜¯æ­£å¸¸çš„ï¼š
```bash
kubectl get application cluster-bootstrap -n argocd
# é æœŸè¼¸å‡º: OutOfSync, Progressing â³
```

2. **æ‰‹å‹•åŒæ­¥åŸºç¤è¨­æ–½ Applications**ï¼ˆæŒ‰é †åºï¼‰:
```bash
# åœ¨ ArgoCD UI ä¸­é»æ“Š SYNCï¼Œæˆ–ä½¿ç”¨ CLI:
kubectl patch application infra-cert-manager -n argocd \
  -p='{"operation":{"sync":{"prune":true}}}' --type=merge

kubectl patch application infra-ingress-nginx -n argocd \
  -p='{"operation":{"sync":{"prune":true}}}' --type=merge
```

3. **ç­‰å¾… CRDs å®‰è£**:
```bash
# æª¢æŸ¥ cert-manager CRDs
kubectl get crd | grep cert-manager
# é æœŸ: certificates.cert-manager.io, clusterissuers.cert-manager.io

# æª¢æŸ¥ ingress-nginx CRDs
kubectl get ingressclass
# é æœŸ: nginx
```

4. **é©—è­‰ cluster-bootstrap è‡ªå‹•é‡è©¦æˆåŠŸ**:
```bash
# ç­‰å¾… 1-2 åˆ†é˜å¾Œæª¢æŸ¥
kubectl get application cluster-bootstrap -n argocd
# é æœŸè¼¸å‡º: Synced, Healthy âœ…
```

**é—œéµé…ç½®**ï¼ˆå·²å…§å»ºï¼‰:
```yaml
# argocd/bootstrap/manifests/*.yaml
metadata:
  annotations:
    argocd.argoproj.io/sync-options: SkipDryRunOnMissingResource=true
    argocd.argoproj.io/sync-wave: "10"
```

**æ™‚é–“ç·š**:
```
T+0min:  cluster-bootstrap éƒ¨ç½² â†’ Phase 1 æˆåŠŸ, Phase 2 å¤±æ•—ï¼ˆCRDs ä¸å­˜åœ¨ï¼‰
T+5min:  æ‰‹å‹•åŒæ­¥åŸºç¤è¨­æ–½ â†’ cert-manager, ingress-nginx å®‰è£
T+7min:  cluster-bootstrap è‡ªå‹•é‡è©¦ â†’ Phase 2 æˆåŠŸ âœ…
```

---

### Q4: ArgoCD Applications é¡¯ç¤º Unknown ç‹€æ…‹ä¸”ä¸è‡ªå‹•åŒæ­¥

**ç—‡ç‹€**:
```
NAME                              SYNC STATUS   HEALTH STATUS
infra-cert-manager                Unknown       Healthy
infra-ingress-nginx               Unknown       Unknown
```

**å¯èƒ½åŸå› **:
1. ApplicationSet å‰›ç”Ÿæˆ Applicationsï¼Œå°šæœªè§¸ç™¼é¦–æ¬¡åŒæ­¥
2. ArgoCD repo-server æœ‰éŒ¯èª¤
3. Git repository èªè­‰å¤±æ•—

**è¨ºæ–·æ­¥é©Ÿ**:

```bash
# 1. æª¢æŸ¥ repo-server æ—¥èªŒ
kubectl logs -n argocd -l app.kubernetes.io/name=argocd-repo-server --tail=50 | grep -i error

# 2. æª¢æŸ¥ Application è©³ç´°ç‹€æ…‹
kubectl get application infra-cert-manager -n argocd -o yaml | grep -A 10 "conditions:"

# 3. æª¢æŸ¥ Git repository é€£æ¥
kubectl get application root -n argocd -o yaml | grep -A 5 "repoURL"
```

**è§£æ±ºæ–¹æ¡ˆ**:

**æ–¹æ¡ˆ A: æ‰‹å‹•è§¸ç™¼åŒæ­¥**
```bash
# åˆ·æ–° root application
kubectl patch application root -n argocd \
  -p='{"metadata":{"annotations":{"argocd.argoproj.io/refresh":"hard"}}}' --type=merge

# ç­‰å¾… 30 ç§’
sleep 30

# æª¢æŸ¥ç‹€æ…‹
kubectl get applications -n argocd
```

**æ–¹æ¡ˆ B: å¦‚æœæ˜¯ Git SSH èªè­‰å•é¡Œ**
```bash
# æª¢æŸ¥ SSH secret æ˜¯å¦å­˜åœ¨
kubectl get secret detectviz-gitops-repo -n argocd

# å¦‚æœä¸å­˜åœ¨ï¼Œå‰µå»º secretï¼ˆéœ€è¦å…ˆæœ‰ SSH ç§é‘°ï¼‰
kubectl create secret generic detectviz-gitops-repo \
  --from-file=sshPrivateKey=/path/to/ssh/key \
  -n argocd

kubectl label secret detectviz-gitops-repo \
  argocd.argoproj.io/secret-type=repository -n argocd

kubectl patch secret detectviz-gitops-repo -n argocd \
  -p='{"stringData":{"type":"git","url":"git@github.com:detectviz/detectviz-gitops.git"}}'
```

**æ–¹æ¡ˆ C: é‡å•Ÿ repo-serverï¼ˆæœ€å¾Œæ‰‹æ®µï¼‰**
```bash
kubectl rollout restart deployment argocd-repo-server -n argocd
kubectl rollout status deployment argocd-repo-server -n argocd --timeout=60s
```

---

### Q5: Kustomize build å¤±æ•—ï¼Œæç¤º "unknown field buildOptions" æˆ–éœ€è¦ --enable-helm

**å®Œæ•´éŒ¯èª¤è¨Šæ¯**:
```
`kustomize build` failed: Error: invalid Kustomization: json: unknown field "buildOptions"
æˆ–
Error: accumulating resources: accumulation err='accumulating resources from 'helmCharts':
must build at root': must specify --enable-helm
```

**åŸå› **:
- Kustomize ä¸æ”¯æ´ `buildOptions` æ¬„ä½ï¼ˆé€™æ˜¯ ArgoCD ç‰¹æœ‰é…ç½®ï¼‰
- Kustomize è™•ç† Helm charts éœ€è¦ `--enable-helm` æ¨™èªŒ

**è§£æ±ºæ–¹æ¡ˆ**:

**éŒ¯èª¤é…ç½®** âŒ:
```yaml
# argocd/appsets/appset.yaml
source:
  path: "{{.path}}"
  kustomize:
    buildOptions: "--enable-helm"  # âŒ Kustomize ä¸èªè­˜æ­¤æ¬„ä½
```

**æ­£ç¢ºé…ç½®** âœ…:

**æ–¹æ¡ˆ A: ä½¿ç”¨ ArgoCD å…¨å±€é…ç½®**ï¼ˆæ¨è–¦ï¼‰
```yaml
# argocd/apps/infrastructure/argocd/overlays/argocd-cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: argocd-cm
  namespace: argocd
data:
  kustomize.buildOptions: "--enable-helm"  # âœ… å…¨å±€å•Ÿç”¨
```

**æ–¹æ¡ˆ B: èª¿æ•´ Kustomize çµæ§‹**ï¼ˆä¸ä½¿ç”¨ Helmï¼‰
```yaml
# base/kustomization.yaml
resources:
  - namespace.yaml
  - deployment.yaml

# ç§»é™¤ helmCharts å€å¡Šï¼Œæ”¹ç”¨ Helm Application
```

**é©—è­‰ä¿®å¾©**:
```bash
# æ¸¬è©¦æœ¬åœ° kustomize build
cd argocd/apps/infrastructure/cert-manager/overlays
kustomize build .

# å¦‚æœæˆåŠŸï¼Œæäº¤ä¿®æ”¹
git add .
git commit -m "fix: Remove invalid kustomize buildOptions"
git push
```

---

## TopoLVM å­˜å„²å•é¡Œ

### Q6: Vault Pods ç„¡æ³•èª¿åº¦ï¼Œé¡¯ç¤º "Insufficient topolvm.io/capacity"

**å®Œæ•´éŒ¯èª¤è¨Šæ¯**:
```
0/4 nodes are available:
1 Insufficient topolvm.io/capacity,
3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }

å¯¦éš›ç¯€é»å®¹é‡: 240GB
PVC éœ€æ±‚: 45GB (10Gi + 10Gi + 10Gi + 5Gi + 5Gi + 5Gi)
```

**æ ¹æœ¬åŸå› **: TopoLVM ä½¿ç”¨äº† Scheduler Extender æ¨¡å¼ï¼Œä½† kube-scheduler æœªé…ç½® extender endpointï¼Œå°è‡´ï¼š
- Webhook æ³¨å…¥éŒ¯èª¤çš„å®¹é‡å€¼ `topolvm.io/capacity: "1"` (åƒ… 1 byte)
- Scheduler èªç‚ºç¯€é»å®¹é‡ä¸è¶³

**è¨ºæ–·æ­¥é©Ÿ**:

```bash
# 1. æª¢æŸ¥ Pod è³‡æºè«‹æ±‚ï¼ˆæ‡‰è©²çœ‹åˆ°éŒ¯èª¤çš„å®¹é‡å€¼ï¼‰
kubectl get pod vault-0 -n vault -o yaml | grep "topolvm.io/capacity"
# éŒ¯èª¤è¼¸å‡º: topolvm.io/capacity: "1"  âŒ (åƒ… 1 byte!)

# 2. æª¢æŸ¥ç¯€é» annotationï¼ˆå¯¦éš›å®¹é‡æ˜¯æ­£ç¢ºçš„ï¼‰
kubectl get node app-worker -o jsonpath='{.metadata.annotations}' | jq 'with_entries(select(.key | contains("topolvm")))'
# æ­£ç¢ºè¼¸å‡º: capacity.topolvm.io/00default: "257693843456"  âœ… (240GB)

# 3. æª¢æŸ¥ CSIStorageCapacity è³‡æº
kubectl get csistoragecapacity -A
# èˆŠæ¨¡å¼: No resources found  âŒ
# æ–°æ¨¡å¼: æ‡‰è©²é¡¯ç¤º topolvm-provisioner å®¹é‡ âœ…

# 4. æª¢æŸ¥æ˜¯å¦æœ‰ scheduler extender DaemonSet
kubectl get daemonset -n kube-system topolvm-scheduler
# èˆŠæ¨¡å¼: å­˜åœ¨ä½†ç„¡ä½œç”¨ âŒ
# æ–°æ¨¡å¼: NotFound âœ…
```

**è§£æ±ºæ–¹æ¡ˆ**: æ”¹ç”¨ **Storage Capacity Tracking** æ¨¡å¼ï¼ˆKubernetes 1.21+ åŸç”ŸåŠŸèƒ½ï¼‰

ä¿®æ­£ `argocd/apps/infrastructure/topolvm/overlays/values.yaml`:
```yaml
# --- TopoLVM Helm Values ---

# 1. ç¦ç”¨ Scheduler Extender
scheduler:
  enabled: false  # âœ… ä¸éœ€è¦ scheduler extender DaemonSet

# 2. å•Ÿç”¨ Storage Capacity Tracking
controller:
  storageCapacityTracking:
    enabled: true  # âœ… ä½¿ç”¨ Kubernetes åŸç”ŸåŠŸèƒ½

# 3. ç¦ç”¨ Pod Mutating Webhook
webhook:
  podMutatingWebhook:
    enabled: false  # âœ… Storage Capacity æ¨¡å¼ä¸éœ€è¦æ³¨å…¥è³‡æºè«‹æ±‚

# 4. å…¶ä»–é…ç½®ä¿æŒä¸è®Š
lvmd:
  deviceClasses:
    - name: ssd
      volume-group: topolvm-vg
      default: true
```

**æäº¤ä¿®æ”¹ä¸¦é‡æ–°éƒ¨ç½²**:
```bash
git add argocd/apps/infrastructure/topolvm/overlays/values.yaml
git commit -m "fix: Enable TopoLVM StorageCapacity tracking instead of scheduler extender"
git push

# åœ¨ ArgoCD UI ä¸­åŒæ­¥ infra-topolvmï¼Œæˆ–ä½¿ç”¨ CLI:
kubectl patch application infra-topolvm -n argocd \
  -p='{"operation":{"sync":{"prune":true}}}' --type=merge
```

**é©—è­‰ä¿®å¾©**:

```bash
# 1. ç­‰å¾… TopoLVM é‡æ–°éƒ¨ç½²
kubectl get pods -n topolvm-system --watch

# 2. æª¢æŸ¥ CSIStorageCapacity è³‡æºï¼ˆæ‡‰è©²å‡ºç¾ï¼‰
kubectl get csistoragecapacity -A
# é æœŸè¼¸å‡º:
# NAMESPACE      NAME                           STORAGECLASS           CAPACITY
# kube-system    topolvm-app-worker-<hash>      topolvm-provisioner    257693843456

# 3. æª¢æŸ¥ scheduler DaemonSet å·²ç§»é™¤
kubectl get daemonset -n kube-system topolvm-scheduler
# é æœŸ: Error from server (NotFound)  âœ…

# 4. åˆªé™¤èˆŠ Vault podsï¼ˆæ¸…é™¤èˆŠ webhook mutationsï¼‰
kubectl delete pod -n vault --all

# 5. æª¢æŸ¥æ–° pods æ˜¯å¦æˆåŠŸèª¿åº¦
kubectl get pods -n vault -o wide
# é æœŸ: Running ç‹€æ…‹ï¼Œèª¿åº¦åˆ° app-worker âœ…

# 6. æª¢æŸ¥ PVC ç¶å®šç‹€æ…‹
kubectl get pvc -n vault
# é æœŸ: æ‰€æœ‰ PVC éƒ½æ˜¯ Bound ç‹€æ…‹ âœ…
```

**ç‚ºä»€éº¼é€™å€‹æ–¹æ¡ˆæ›´å¥½**:
- âœ… Kubernetes åŸç”ŸåŠŸèƒ½ï¼ˆ1.21+ GAï¼‰
- âœ… ç„¡éœ€ä¿®æ”¹ kube-scheduler é…ç½®
- âœ… ç„¡éœ€ webhook æ³¨å…¥è³‡æºè«‹æ±‚
- âœ… è‡ªå‹•å®¹é‡è¿½è¹¤å’Œæ›´æ–°
- âœ… æ›´ç°¡å–®ã€æ›´å¯é çš„èª¿åº¦æ©Ÿåˆ¶

**åƒè€ƒæ–‡æª”**:
- [TopoLVM Storage Capacity Tracking](https://github.com/topolvm/topolvm/blob/main/docs/design.md#storage-capacity-tracking)
- [Kubernetes CSI Storage Capacity](https://kubernetes.io/docs/concepts/storage/storage-capacity/)

---

### Q7: TopoLVM PVC ä¸€ç›´è™•æ–¼ Pending ç‹€æ…‹

**ç—‡ç‹€**:
```bash
kubectl get pvc -n vault
NAME              STATUS    VOLUME   CAPACITY   STORAGECLASS
data-vault-0      Pending                        topolvm-provisioner
```

**å¯èƒ½åŸå› **:

**åŸå›  1: VolumeBindingMode ç‚º WaitForFirstConsumer**
```bash
# æª¢æŸ¥ StorageClass
kubectl get storageclass topolvm-provisioner -o yaml | grep volumeBindingMode
# è¼¸å‡º: volumeBindingMode: WaitForFirstConsumer
```

**è§£æ±ºæ–¹æ¡ˆ**: é€™æ˜¯æ­£å¸¸çš„ï¼PVC æœƒç­‰åˆ° Pod è¢«èª¿åº¦å¾Œæ‰å‰µå»º PVã€‚

é©—è­‰ Pod ç‹€æ…‹:
```bash
kubectl get pods -n vault
# å¦‚æœ Pod æ˜¯ Runningï¼ŒPVC æ‡‰è©²è®Šæˆ Bound
# å¦‚æœ Pod æ˜¯ Pendingï¼Œæª¢æŸ¥ Pod çš„èª¿åº¦å•é¡Œ
```

**åŸå›  2: Volume Group ä¸å­˜åœ¨æˆ–åç¨±ä¸åŒ¹é…**
```bash
# æª¢æŸ¥ TopoLVM values
kubectl get configmap -n kube-system topolvm-lvmd -o yaml | grep volume-group

# SSH åˆ° worker ç¯€é»æª¢æŸ¥ VG
ssh ubuntu@192.168.0.14 'sudo vgs'
# é æœŸçœ‹åˆ°: topolvm-vg
```

**è§£æ±ºæ–¹æ¡ˆ**: ç¢ºä¿ VG åç¨±ä¸€è‡´

```bash
# å¦‚æœ VG åç¨±ä¸å°ï¼Œä¿®æ­£ values.yaml
# argocd/apps/infrastructure/topolvm/overlays/values.yaml
lvmd:
  deviceClasses:
    - name: ssd
      volume-group: topolvm-vg  # âœ… å¿…é ˆèˆ‡å¯¦éš› VG åç¨±ä¸€è‡´
```

**åŸå›  3: ç£ç¢Ÿç©ºé–“ä¸è¶³**
```bash
# æª¢æŸ¥ VG å‰©é¤˜ç©ºé–“
ssh ubuntu@192.168.0.14 'sudo vgs'
# æŸ¥çœ‹ VFree æ¬„ä½

# å¦‚æœç©ºé–“ä¸è¶³ï¼Œéœ€è¦ï¼š
# 1. åˆªé™¤ä¸éœ€è¦çš„ PVC
kubectl delete pvc <unused-pvc> -n <namespace>

# 2. æˆ–è€…æ·»åŠ æ›´å¤šç£ç¢Ÿåˆ° VG
```

---

### Q8: TopoLVM Volume Group åç¨±é…ç½®ä¸ä¸€è‡´

**ç—‡ç‹€**: é…ç½®æ–‡ä»¶ä¸­å‡ºç¾ä¸åŒçš„ VG åç¨±ï¼ˆ`data-vg`, `nvme-vg`, `topolvm-vg`ï¼‰

**æª¢æŸ¥é…ç½®ä¸€è‡´æ€§**:

```bash
# 1. æª¢æŸ¥ Ansible é…ç½®
grep -r "lvm_volume_groups" ansible/group_vars/

# 2. æª¢æŸ¥ TopoLVM values
grep "volume-group" argocd/apps/infrastructure/topolvm/overlays/values.yaml

# 3. æª¢æŸ¥å¯¦éš› VG åç¨±
ssh ubuntu@192.168.0.14 'sudo vgs --noheadings -o vg_name | grep -v ubuntu'

# 4. æª¢æŸ¥æ–‡æª”
grep -n "topolvm-vg\|data-vg\|nvme-vg" deploy.md
```

**æ¨™æº–é…ç½®**ï¼ˆç•¶å‰çµ±ä¸€ç‚º `topolvm-vg`ï¼‰:

| æª”æ¡ˆ | é…ç½®é … | å€¼ |
|------|--------|-----|
| `ansible/group_vars/all.yml` | `lvm_volume_groups[0].name` | `topolvm-vg` |
| `argocd/apps/infrastructure/topolvm/overlays/values.yaml` | `lvmd.deviceClasses[0].volume-group` | `topolvm-vg` |
| `deploy.md` | æ–‡æª”èªªæ˜ | `topolvm-vg` |

**ä¿®æ­£æ­¥é©Ÿ**:

1. **çµ±ä¸€é…ç½®æ–‡ä»¶**:
```bash
# ç¢ºä¿æ‰€æœ‰å¼•ç”¨éƒ½æ˜¯ topolvm-vg
rg "data-vg|nvme-vg" --type yaml

# å¦‚æœæœ‰ç™¼ç¾ï¼Œé€ä¸€ä¿®æ­£
```

2. **å¦‚æœå¯¦éš› VG åç¨±ä¸åŒï¼Œéœ€è¦é‡å»º**:
```bash
# âš ï¸ è­¦å‘Šï¼šæ­¤æ“ä½œæœƒåˆªé™¤æ‰€æœ‰æ•¸æ“šï¼
ssh ubuntu@192.168.0.14 'sudo vgremove <old-vg-name>'
ssh ubuntu@192.168.0.14 'sudo vgcreate topolvm-vg /dev/sdb'
```

3. **é©—è­‰ä¸€è‡´æ€§**:
```bash
# é‹è¡Œé©—è­‰è…³æœ¬
cat > /tmp/check-topolvm-vg.sh << 'EOF'
#!/bin/bash
echo "=== Checking TopoLVM VG Configuration ==="

echo "1. Ansible config:"
grep -A 2 "lvm_volume_groups:" ansible/group_vars/all.yml | grep "name:"

echo "2. TopoLVM Helm values:"
grep "volume-group:" argocd/apps/infrastructure/topolvm/overlays/values.yaml

echo "3. Actual VG on worker:"
ssh ubuntu@192.168.0.14 'sudo vgs --noheadings -o vg_name | grep topolvm'

echo "4. Documentation:"
grep -c "topolvm-vg" deploy.md
EOF

chmod +x /tmp/check-topolvm-vg.sh
/tmp/check-topolvm-vg.sh
```

---

## ç¶²è·¯èˆ‡ DNS å•é¡Œ

### Q9: VM ä¹‹é–“ç„¡æ³•é€šé cluster.internal åŸŸåè§£æ

**ç—‡ç‹€**:
```bash
ssh ubuntu@192.168.0.11 'getent hosts master-2.cluster.internal'
# ç„¡è¼¸å‡ºæˆ–éŒ¯èª¤
```

**è¨ºæ–·æ­¥é©Ÿ**:

```bash
# 1. æª¢æŸ¥ VM çš„ /etc/resolv.conf
ssh ubuntu@192.168.0.11 'cat /etc/resolv.conf'
# æ‡‰è©²åŒ…å«: nameserver 192.168.0.2

# 2. æª¢æŸ¥ /etc/hosts
ssh ubuntu@192.168.0.11 'cat /etc/hosts | grep cluster.internal'

# 3. æ¸¬è©¦ DNS æœå‹™å™¨
ssh ubuntu@192.168.0.11 'dig @192.168.0.2 master-2.cluster.internal +short'

# 4. æª¢æŸ¥ Proxmox dnsmasq é…ç½®
ssh root@192.168.0.2 'cat /etc/dnsmasq.d/detectviz.conf | grep cluster.internal'
```

**è§£æ±ºæ–¹æ¡ˆ**:

**æ–¹æ¡ˆ A: ä¿®æ­£ dnsmasq é…ç½®**ï¼ˆå¦‚æœç¼ºå°‘ cluster.internal è¨˜éŒ„ï¼‰
```bash
# åœ¨ Proxmox ä¸Šç·¨è¼¯
ssh root@192.168.0.2

cat >> /etc/dnsmasq.d/detectviz.conf << 'EOF'
# å…§éƒ¨é›†ç¾¤ç¶²è·¯åŸŸå
local=/cluster.internal/

# å…§éƒ¨ç¶²è·¯è¨˜éŒ„ (vmbr1 - 10.0.0.0/24)
address=/master-1.cluster.internal/10.0.0.11
address=/master-2.cluster.internal/10.0.0.12
address=/master-3.cluster.internal/10.0.0.13
address=/app-worker.cluster.internal/10.0.0.14
EOF

# é‡å•Ÿ dnsmasq
systemctl restart dnsmasq
systemctl status dnsmasq
```

**æ–¹æ¡ˆ B: ä½¿ç”¨ Ansible é‡æ–°é…ç½®ç¶²è·¯**
```bash
# é‡æ–°é‹è¡Œç¶²è·¯é…ç½® playbook
cd ansible/
ansible-playbook -i inventory.ini deploy-cluster.yml --tags network

# é©—è­‰
ansible all -i inventory.ini -m shell -a "cat /etc/hosts | grep cluster.internal"
```

---

### Q10: MTU è¨­å®šå•é¡Œå°è‡´ç¶²è·¯ä¸ç©©å®š

**ç—‡ç‹€**:
- è¨­å®š MTU 9000 å¾Œ VM ç„¡æ³•é€£ç·š
- å¤§å°åŒ…å‚³è¼¸å¤±æ•—
- SSH é€£æ¥ä¸ç©©å®š

**è¨ºæ–·æ­¥é©Ÿ**:

```bash
# 1. æ¸¬è©¦æ¨™æº– MTU (1472 + 28 = 1500)
ping -c 3 -M do -s 1472 192.168.0.11
# æ‡‰è©²æˆåŠŸ

# 2. æ¸¬è©¦å·¨å‹å¹€ MTU (8972 + 28 = 9000)
ping -c 3 -M do -s 8972 192.168.0.11
# å¦‚æœå¤±æ•—ï¼Œè¡¨ç¤ºä¸æ”¯æ´ MTU 9000

# 3. é€æ­¥æ¸¬è©¦æ‰¾å‡ºæœ€å¤§ MTU
ping -c 3 -M do -s 3972 192.168.0.11  # MTU 4000
ping -c 3 -M do -s 7972 192.168.0.11  # MTU 8000

# 4. æª¢æŸ¥ç•¶å‰ MTU è¨­å®š
ssh ubuntu@192.168.0.11 'ip link show eth0 | grep mtu'
ssh ubuntu@192.168.0.11 'ip link show eth1 | grep mtu'
```

**è§£æ±ºæ–¹æ¡ˆ**: æ”¹å› MTU 1500

**æ­¥é©Ÿ 1: ä¿®æ­£ Proxmox ç¶²è·¯é…ç½®**
```bash
ssh root@192.168.0.2

# ç·¨è¼¯ç¶²è·¯é…ç½®
vi /etc/network/interfaces

# ä¿®æ”¹ MTU
auto vmbr0
iface vmbr0 inet static
    ...
    mtu 1500  # âœ… æ”¹å›æ¨™æº– MTU

auto vmbr1
iface vmbr1 inet static
    ...
    mtu 1500  # âœ… æ”¹å›æ¨™æº– MTU

# é‡å•Ÿç¶²è·¯
systemctl restart networking
```

**æ­¥é©Ÿ 2: ä¿®æ­£ Terraform é…ç½®**
```bash
# ç·¨è¼¯ terraform/terraform.tfvars
vi terraform/terraform.tfvars

# ä¿®æ”¹ MTU è¨­å®š
proxmox_mtu = 1500  # âœ… æ¨™æº– MTU
```

**æ­¥é©Ÿ 3: ä½¿ç”¨ Ansible é‡æ–°é…ç½® VM ç¶²è·¯**
```bash
cd ansible/
ansible-playbook -i inventory.ini deploy-cluster.yml --tags network

# é©—è­‰ MTU
ansible all -i inventory.ini -m shell -a "ip link show | grep mtu"
# æ‰€æœ‰ä»‹é¢æ‡‰è©²é¡¯ç¤º mtu 1500
```

**MTU æœ€ä½³å¯¦è¸**:
- âœ… **æ¨™æº–ç’°å¢ƒ**: ä½¿ç”¨ MTU 1500ï¼ˆé©ç”¨æ–¼æ‰€æœ‰ç¡¬é«”ï¼‰
- âš ï¸ **ä¼æ¥­ç´šç’°å¢ƒ**: MTU 9000 éœ€è¦æ•´æ¢è·¯å¾‘éƒ½æ”¯æ´ï¼ˆç¶²å¡ã€äº¤æ›æ©Ÿã€ç·šæï¼‰
- ğŸ“Š **æ€§èƒ½å½±éŸ¿**: å°æ–¼ Kubernetes å°å‹é›†ç¾¤ï¼ŒMTU 1500 vs 9000 å·®ç•°å¯å¿½ç•¥

---

## Ansible éƒ¨ç½²å•é¡Œ

### Q11: Ansible ä»»å‹™å¤±æ•—ï¼Œæç¤ºæ¬Šé™ä¸è¶³

**éŒ¯èª¤è¨Šæ¯**:
```
TASK [common : Install required packages] *****
fatal: [master-1]: FAILED! => {"msg": "This task requires superuser privileges"}
```

**åŸå› **: Ansible ä»»å‹™ç¼ºå°‘ `become: true`

**è§£æ±ºæ–¹æ¡ˆ**:

æª¢æŸ¥ä¸¦ä¿®æ­£ Ansible role:
```yaml
# ansible/roles/common/tasks/main.yml
---
- name: Install required packages
  become: true  # âœ… æ·»åŠ æ­¤è¡Œ
  apt:
    name:
      - apt-transport-https
      - ca-certificates
      - curl
    state: present
    update_cache: yes
```

**æ‰¹é‡æª¢æŸ¥**:
```bash
# æŸ¥æ‰¾æ‰€æœ‰ç¼ºå°‘ become çš„ apt ä»»å‹™
grep -r "^- name:" ansible/roles/*/tasks/main.yml | while read line; do
  file=$(echo $line | cut -d: -f1)
  grep -A 5 "$line" "$file" | grep -q "become: true" || echo "Missing become in: $file"
done
```

---

### Q12: Ansible è®Šæ•¸æœªå®šç¾©éŒ¯èª¤

**éŒ¯èª¤è¨Šæ¯**:
```
fatal: [master-1]: FAILED! => {"msg": "The task includes an option with an undefined variable.
The error was: 'domain' is undefined"}
```

**è¨ºæ–·æ­¥é©Ÿ**:

```bash
# 1. æª¢æŸ¥è®Šæ•¸å®šç¾©ä½ç½®
grep -r "domain:" ansible/group_vars/
grep -r "domain:" ansible/host_vars/

# 2. æª¢æŸ¥è®Šæ•¸å¼•ç”¨
grep -r "{{ domain }}" ansible/roles/
```

**è§£æ±ºæ–¹æ¡ˆ**:

**é¸é … A: æ·»åŠ ç¼ºå¤±çš„è®Šæ•¸**
```yaml
# ansible/group_vars/all.yml
---
domain: detectviz.internal
cluster_domain: cluster.internal
```

**é¸é … B: ä½¿ç”¨æ¢ä»¶æª¢æŸ¥**
```yaml
# ansible/roles/example/tasks/main.yml
- name: Configure domain
  when: domain is defined
  template:
    src: config.j2
    dest: /etc/app/config.yaml
```

**é¸é … C: æä¾›é»˜èªå€¼**
```yaml
# Jinja2 template
domain: {{ domain | default('example.local') }}
```

---

## Vault ç›¸é—œå•é¡Œ

### Q13: Vault Pods å•Ÿå‹•å¾Œç«‹å³é¡¯ç¤º "not ready"

**ç—‡ç‹€**:
```bash
kubectl get pods -n vault
NAME      READY   STATUS    RESTARTS   AGE
vault-0   0/1     Running   0          2m
vault-1   0/1     Running   0          2m
vault-2   0/1     Running   0          2m
```

**åŸå› **: Vault éœ€è¦æ‰‹å‹•åˆå§‹åŒ–å’Œè§£å°

**é€™æ˜¯æ­£å¸¸çš„ï¼** Vault çš„å®‰å…¨è¨­è¨ˆè¦æ±‚æ‰‹å‹•åˆå§‹åŒ–ã€‚

**è§£æ±ºæ­¥é©Ÿ**:

**1. åˆå§‹åŒ–ç¬¬ä¸€å€‹ Vault å¯¦ä¾‹**:
```bash
kubectl exec -n vault vault-0 -c vault -- vault operator init \
  -key-shares=5 \
  -key-threshold=3 \
  -format=json > vault-keys.json

# âš ï¸ é‡è¦ï¼šå®‰å…¨ä¿å­˜ vault-keys.jsonï¼
chmod 600 vault-keys.json
```

**2. è§£å°æ‰€æœ‰ Vault å¯¦ä¾‹**:
```bash
# æå– unseal keys
UNSEAL_KEY_1=$(jq -r '.unseal_keys_b64[0]' vault-keys.json)
UNSEAL_KEY_2=$(jq -r '.unseal_keys_b64[1]' vault-keys.json)
UNSEAL_KEY_3=$(jq -r '.unseal_keys_b64[2]' vault-keys.json)

# è§£å° vault-0ï¼ˆéœ€è¦ 3 å€‹ keysï¼‰
kubectl exec -n vault vault-0 -c vault -- vault operator unseal $UNSEAL_KEY_1
kubectl exec -n vault vault-0 -c vault -- vault operator unseal $UNSEAL_KEY_2
kubectl exec -n vault vault-0 -c vault -- vault operator unseal $UNSEAL_KEY_3

# è§£å° vault-1
kubectl exec -n vault vault-1 -c vault -- vault operator unseal $UNSEAL_KEY_1
kubectl exec -n vault vault-1 -c vault -- vault operator unseal $UNSEAL_KEY_2
kubectl exec -n vault vault-1 -c vault -- vault operator unseal $UNSEAL_KEY_3

# è§£å° vault-2
kubectl exec -n vault vault-2 -c vault -- vault operator unseal $UNSEAL_KEY_1
kubectl exec -n vault vault-2 -c vault -- vault operator unseal $UNSEAL_KEY_2
kubectl exec -n vault vault-2 -c vault -- vault operator unseal $UNSEAL_KEY_3
```

**3. é©—è­‰ç‹€æ…‹**:
```bash
# æª¢æŸ¥æ‰€æœ‰ pods
kubectl get pods -n vault
# é æœŸ: æ‰€æœ‰ pods READY 1/1

# æª¢æŸ¥ Vault ç‹€æ…‹
kubectl exec -n vault vault-0 -c vault -- vault status
# é æœŸ: Sealed: false
```

**è‡ªå‹•åŒ–è…³æœ¬**:
```bash
# å‰µå»ºè§£å°è…³æœ¬ä¾›å¾ŒçºŒä½¿ç”¨
cat > unseal-vault.sh << 'EOF'
#!/bin/bash
KEYS_FILE=${1:-vault-keys.json}

if [ ! -f "$KEYS_FILE" ]; then
  echo "Error: $KEYS_FILE not found"
  exit 1
fi

UNSEAL_KEY_1=$(jq -r '.unseal_keys_b64[0]' $KEYS_FILE)
UNSEAL_KEY_2=$(jq -r '.unseal_keys_b64[1]' $KEYS_FILE)
UNSEAL_KEY_3=$(jq -r '.unseal_keys_b64[2]' $KEYS_FILE)

for pod in vault-0 vault-1 vault-2; do
  echo "Unsealing $pod..."
  kubectl exec -n vault $pod -c vault -- vault operator unseal $UNSEAL_KEY_1
  kubectl exec -n vault $pod -c vault -- vault operator unseal $UNSEAL_KEY_2
  kubectl exec -n vault $pod -c vault -- vault operator unseal $UNSEAL_KEY_3
done

echo "Vault unsealed successfully!"
EOF

chmod +x unseal-vault.sh
```

---

## å¿«é€Ÿåƒè€ƒ

### å¸¸ç”¨è¨ºæ–·å‘½ä»¤

```bash
# ArgoCD ç‹€æ…‹æª¢æŸ¥
kubectl get applications -n argocd
kubectl get applicationset -n argocd
kubectl logs -n argocd -l app.kubernetes.io/name=argocd-server --tail=50

# TopoLVM ç‹€æ…‹æª¢æŸ¥
kubectl get pods -n topolvm-system
kubectl get csistoragecapacity -A
kubectl get storageclass topolvm-provisioner -o yaml

# é›†ç¾¤å¥åº·æª¢æŸ¥
kubectl get nodes -o wide
kubectl get pods -A --field-selector=status.phase!=Running
kubectl top nodes

# ç¶²è·¯è¨ºæ–·
ip addr show
ip route
ping -c 3 -M do -s 1472 <target-ip>

# å­˜å„²è¨ºæ–·
ssh ubuntu@192.168.0.14 'sudo vgs && sudo pvs && sudo lvs'
kubectl get pvc -A
kubectl get pv
```

### ç›¸é—œæ–‡æª”

- ä¸»éƒ¨ç½²æ–‡æª”: `deploy.md`
- ç¶²è·¯é…ç½®: `docs/infrastructure/00-planning/configuration-network.md`
- åŸŸåé…ç½®: `docs/infrastructure/00-planning/configuration-domain.md`
- å­˜å„²é…ç½®: `docs/infrastructure/00-planning/configuration-storage.md`
- ArgoCD Bootstrap: `argocd/bootstrap/PHASE_DEPLOYMENT.md`

---

**æœ€å¾Œæ›´æ–°**: 2025-11-14
**åŸºæ–¼**: deploy.log å¯¦éš›éƒ¨ç½²æ—¥èªŒåˆ†æ
