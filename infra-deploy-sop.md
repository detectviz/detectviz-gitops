# DetectViz GitOps Infrastructure Deployment Guide

**åŸºæ–¼æ¶æ§‹**: `README.md` (4 VM æ··åˆè² è¼‰æ¨¡å‹ + é›™ç¶²è·¯æ¶æ§‹)

æœ¬æ–‡ä»¶æä¾›å®Œæ•´çš„éƒ¨ç½²æµç¨‹ï¼Œå¾ Proxmox ç¶²è·¯é…ç½®åˆ° Kubernetes é›†ç¾¤å•Ÿå‹•çš„æ‰€æœ‰æ­¥é©Ÿã€‚æ•…éšœæ’é™¤ç´€éŒ„åœ¨ `infra-deploy-troubleshooting.md` ä¸­ã€‚

---

## ç›®éŒ„

- [Phase 0: å‰ç½®ä½œæ¥­](#phase-0-å‰ç½®ä½œæ¥­)
  - [1. Proxmox é›™ç¶²è·¯é…ç½®](#1-proxmox-é›™ç¶²è·¯é…ç½®)
  - [2. DNS ä¼ºæœå™¨é…ç½®](#2-dns-ä¼ºæœå™¨é…ç½®)
  - [3. VM æ¨¡æ¿æº–å‚™](#3-vm-æ¨¡æ¿æº–å‚™)
  - [4. SSH é‡‘é‘°æº–å‚™](#4-ssh-é‡‘é‘°æº–å‚™)
- [Phase 1: Terraform åŸºç¤è¨­æ–½ä½ˆå»º](#phase-1-terraform-åŸºç¤è¨­æ–½ä½ˆå»º)
- [Phase 2: ç¶²è·¯é…ç½®é©—è­‰](#phase-2-ç¶²è·¯é…ç½®é©—è­‰)
- [Phase 3: Ansible è‡ªå‹•åŒ–éƒ¨ç½²](#phase-3-ansible-è‡ªå‹•åŒ–éƒ¨ç½²)
- [Phase 4: GitOps åŸºç¤è¨­æ–½åŒæ­¥](#phase-4-gitops-åŸºç¤è¨­æ–½åŒæ­¥)
- [Phase 5: Vault åˆå§‹åŒ–](#phase-5-vault-åˆå§‹åŒ–)

---

## Phase 0: å‰ç½®ä½œæ¥­

### 1. Proxmox é›™ç¶²è·¯é…ç½®

DetectViz ä½¿ç”¨é›™ç¶²è·¯æ¶æ§‹ä»¥åˆ†é›¢ç®¡ç†æµé‡èˆ‡é›†ç¾¤å…§éƒ¨é€šè¨Šã€‚

**åƒè€ƒæ–‡ä»¶**: `docs/infrastructure/00-planning/configuration-network.md`

#### 1.1 é…ç½®ç¶²è·¯æ©‹æ¥å™¨

ç·¨è¼¯ `/etc/network/interfaces`ï¼š

```bash
# å‚™ä»½ç¾æœ‰é…ç½®
cp /etc/network/interfaces /etc/network/interfaces.backup

# ç·¨è¼¯ç¶²è·¯é…ç½®
vi /etc/network/interfaces
```

**é…ç½®å…§å®¹**ï¼š

```bash
# å¤–éƒ¨ç¶²è·¯æ©‹æ¥å™¨ (vmbr0 - enp4s0)
auto vmbr0
iface vmbr0 inet static
    address 192.168.0.2/24
    gateway 192.168.0.1
    bridge-ports enp4s0
    bridge-stp off
    bridge-fd 0
    mtu 1500

# å…§éƒ¨é›†ç¾¤ç¶²è·¯æ©‹æ¥å™¨ (vmbr1 - enp5s0)
auto vmbr1
iface vmbr1 inet static
    address 10.0.0.2/24
    bridge-ports enp5s0
    bridge-stp off
    bridge-fd 0
    mtu 1500
```

> **MTU è¨­å®šèªªæ˜**:
> - **é è¨­ 1500**: é©ç”¨æ–¼æ‰€æœ‰æ¨™æº–ç¶²å¡å’Œäº¤æ›æ©Ÿï¼Œå»ºè­°ä½¿ç”¨
> - **é€²éš 9000**: éœ€è¦ç¶²å¡ã€äº¤æ›æ©Ÿã€ç·šæå…¨éƒ¨æ”¯æ´å·¨å‹å¹€ï¼ˆJumbo Framesï¼‰ï¼Œå¦å‰‡æœƒå°è‡´é€£ç·šå¤±æ•—
> - **è¨ºæ–·æ–¹æ³•**: å¦‚æœè¨­å®š 9000 å¾Œç„¡æ³•é€£ç·šï¼Œè«‹æ”¹å› 1500

#### 1.2 é…ç½® sysctl åƒæ•¸

```bash
cat <<EOF | tee /etc/sysctl.d/99-proxmox-network.conf
# Proxmox Host Network Configuration
net.ipv4.conf.all.rp_filter = 2
net.ipv4.conf.default.rp_filter = 2
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
net.ipv6.conf.all.disable_ipv6 = 1
EOF

sysctl --system
```

#### 1.3 é‡å•Ÿç¶²è·¯æœå‹™

```bash
systemctl restart networking
```

#### 1.4 é©—è­‰é…ç½®

```bash
# æª¢æŸ¥æ©‹æ¥å™¨ç‹€æ…‹
ip addr show vmbr0
ip addr show vmbr1

# é©—è­‰ MTU
ip link show vmbr0 | grep mtu
ip link show vmbr1 | grep mtu

# é©—è­‰ sysctl
sysctl net.ipv4.ip_forward
sysctl net.ipv4.conf.all.rp_filter
```

**é æœŸçµæœ**ï¼š
- vmbr0: 192.168.0.2/24, MTU 1500
- vmbr1: 10.0.0.2/24, MTU 1500
- ip_forward = 1
- rp_filter = 2

---

### 2. DNS ä¼ºæœå™¨é…ç½®

DetectViz ä½¿ç”¨ Proxmox dnsmasq æä¾›å…§éƒ¨ DNS è§£æã€‚

**åƒè€ƒæ–‡ä»¶**: `docs/infrastructure/00-planning/configuration-domain.md`

#### 2.1 å®‰è£ dnsmasq

```bash
apt update
apt install dnsmasq -y
```

#### 2.2 é…ç½® dnsmasq

å‰µå»º `/etc/dnsmasq.d/detectviz.conf`ï¼š

```bash
cat <<EOF | tee /etc/dnsmasq.d/detectviz.conf
# DetectViz DNS Configuration
domain=detectviz.internal
expand-hosts
local=/detectviz.internal/

# å¤–éƒ¨ç¶²è·¯è¨˜éŒ„ (vmbr0)
address=/proxmox.detectviz.internal/192.168.0.2
address=/ipmi.detectviz.internal/192.168.0.4
address=/k8s-api.detectviz.internal/192.168.0.10
address=/master-1.detectviz.internal/192.168.0.11
address=/master-2.detectviz.internal/192.168.0.12
address=/master-3.detectviz.internal/192.168.0.13
address=/app-worker.detectviz.internal/192.168.0.14

# å…§éƒ¨é›†ç¾¤ç¶²è·¯åŸŸå
local=/cluster.internal/

# å…§éƒ¨ç¶²è·¯è¨˜éŒ„ (vmbr1)
address=/master-1.cluster.internal/10.0.0.11
address=/master-2.cluster.internal/10.0.0.12
address=/master-3.cluster.internal/10.0.0.13
address=/app-worker.cluster.internal/10.0.0.14

# æ‡‰ç”¨æœå‹™
address=/argocd.detectviz.internal/192.168.0.10
address=/grafana.detectviz.internal/192.168.0.10
address=/prometheus.detectviz.internal/192.168.0.10
address=/loki.detectviz.internal/192.168.0.10
address=/tempo.detectviz.internal/192.168.0.10
address=/pgadmin.detectviz.internal/192.168.0.10

# ä¸Šæ¸¸ DNS
server=8.8.8.8
server=1.1.1.1

listen-address=127.0.0.1,192.168.0.2
bind-interfaces
EOF
```

#### 2.3 å•Ÿå‹• dnsmasq

```bash
systemctl enable --now dnsmasq
systemctl restart dnsmasq
systemctl status dnsmasq
```

#### 2.4 é©—è­‰ DNS

```bash
# æ¸¬è©¦å¤–éƒ¨åŸŸåè§£æ
dig @192.168.0.2 master-1.detectviz.internal +short
# é æœŸ: 192.168.0.11

# æ¸¬è©¦é›†ç¾¤å…§éƒ¨åŸŸåè§£æ
dig @192.168.0.2 master-1.cluster.internal +short
# é æœŸ: 10.0.0.11

# æ¸¬è©¦å¤–éƒ¨ DNS è½‰ç™¼
dig @192.168.0.2 google.com +short
# é æœŸ: Google IP ä½å€
```

---

### 3. VM æ¨¡æ¿æº–å‚™

**åƒè€ƒæ–‡ä»¶**: `docs/infrastructure/02-proxmox/vm-template-creation.md`

ç¢ºä¿å·²å»ºç«‹ Ubuntu 22.04 Cloud-init æ¨¡æ¿ï¼ˆVM ID: 9000ï¼‰

é©—è­‰æ¨¡æ¿ï¼š

```bash
pvesh get /nodes/proxmox/qemu --output-format json | jq -r '.[] | select(.template==1) | .name'
# é æœŸè¼¸å‡º: ubuntu-2204-template
```

---

### 4. SSH é‡‘é‘°æº–å‚™

```bash
# æª¢æŸ¥æ˜¯å¦å·²æœ‰ SSH é‡‘é‘°
ls -la ~/.ssh/id_rsa.pub

# å¦‚æœæ²’æœ‰ï¼Œç”Ÿæˆæ–°çš„é‡‘é‘°å°
ssh-keygen -t rsa -b 4096 -C "your_email@example.com"
```

---

## Phase 1: Terraform åŸºç¤è¨­æ–½ä½ˆå»º

**ç›®æ¨™**: å»ºç«‹ 4 å° VMï¼Œé…ç½®é›™ç¶²è·¯æ¶æ§‹ï¼ˆvmbr0 + vmbr1ï¼‰

#### 1.1 æª¢æŸ¥ Terraform é…ç½®

ç¢ºèª `terraform/terraform.tfvars` é…ç½®æ­£ç¢ºï¼š

```bash
cd terraform/

# æª¢æŸ¥ç¶²è·¯é…ç½®
grep -E 'proxmox_bridge|k8s_overlay_bridge|master_internal_ips|worker_internal_ips|cluster_domain' terraform.tfvars

# æª¢æŸ¥ç£ç¢Ÿé…ç½®
grep -E 'worker_system_disk_sizes|worker_data_disks' terraform.tfvars
```

**é æœŸè¼¸å‡º - ç¶²è·¯é…ç½®**ï¼š
```
proxmox_bridge     = "vmbr0"          # å¤–éƒ¨ç¶²è·¯ (ç®¡ç† + æ‡‰ç”¨)
k8s_overlay_bridge = "vmbr1"          # å…§éƒ¨ç¶²è·¯ (Kubernetes ç¯€é»é–“é€šè¨Š)
master_internal_ips = ["10.0.0.11", "10.0.0.12", "10.0.0.13"]
worker_internal_ips = ["10.0.0.14"]
cluster_domain      = "cluster.internal"
```

**é æœŸè¼¸å‡º - ç£ç¢Ÿé…ç½®ï¼ˆé›™ç£ç¢Ÿæ¶æ§‹ï¼‰**ï¼š
```hcl
worker_system_disk_sizes = ["100G"]    # ç³»çµ±ç£ç¢Ÿ (OS + kubelet)
worker_data_disks = [
  {
    size    = "250G"                   # è³‡æ–™ç£ç¢Ÿ (TopoLVM topolvm-vg)
    storage = "nvme-vm"
  }
]
```

**èªªæ˜**ï¼š
- **Master ç¯€é»**: å–®ç£ç¢Ÿ 100GB (OS + etcd)
- **Worker ç¯€é»**: é›™ç£ç¢Ÿæ¶æ§‹
  - `/dev/sda` 100GB: ç³»çµ±ç£ç¢Ÿ
  - `/dev/sdb` 250GB: è³‡æ–™ç£ç¢Ÿ (ä¾› TopoLVM ç®¡ç†ï¼Œå‹•æ…‹ PV)

#### 1.2 åˆå§‹åŒ–ä¸¦éƒ¨ç½²

```bash
terraform init
terraform plan -var-file=terraform.tfvars
terraform apply -auto-approve
```

#### 1.3 é©—è­‰ VM å‰µå»º

```bash
# æª¢æŸ¥ VM ç‹€æ…‹
pvesh get /nodes/proxmox/qemu --output-format json | jq -r '.[] | select(.vmid >= 111 and .vmid <= 114) | {vmid, name, status}'

# æ¸¬è©¦ SSH é€£æ¥
ssh ubuntu@192.168.0.11 'hostname'
ssh ubuntu@192.168.0.14 'hostname'

# æª¢æŸ¥ app-worker ç£ç¢Ÿé…ç½®
ssh ubuntu@192.168.0.14 'lsblk'
```

**é æœŸè¼¸å‡º (app-worker ç£ç¢Ÿ)**ï¼š
```
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sda      8:0    0  100G  0 disk
â”œâ”€sda1   8:1    0    1M  0 part
â”œâ”€sda2   8:2    0    2G  0 part /boot
â””â”€sda3   8:3    0   98G  0 part
  â””â”€ubuntu--vg-ubuntu--lv 253:0 0 98G  0 lvm  /
sdb      8:16   0  250G  0 disk     â† è³‡æ–™ç£ç¢Ÿ (æœªæ ¼å¼åŒ–)
```

#### 1.4 TopoLVM Volume Group é…ç½®

**é‡è¦**: LVM Volume Group çš„å»ºç«‹å·²ç¶“**è‡ªå‹•åŒ–**åœ¨ Ansible éƒ¨ç½²æµç¨‹ä¸­ (Phase 4: Worker Role),**ç„¡éœ€æ‰‹å‹•æ“ä½œ**ã€‚

Ansible æœƒåœ¨ Phase 4 è‡ªå‹•åŸ·è¡Œ:
1. æª¢æŸ¥ /dev/sdb ç£ç¢Ÿæ˜¯å¦å­˜åœ¨
2. å»ºç«‹ Physical Volume (`pvcreate /dev/sdb`)
3. å»ºç«‹ Volume Group (`vgcreate topolvm-vg /dev/sdb`)
4. é©—è­‰ LVM é…ç½®

é…ç½®æª”æ¡ˆä½ç½®: `ansible/group_vars/all.yml:51-60`

```yaml
configure_lvm: true  # å•Ÿç”¨ LVM è‡ªå‹•é…ç½®

lvm_volume_groups:
  - name: topolvm-vg   # Volume Group åç¨±
    devices:
      - /dev/sdb       # ä½¿ç”¨çš„ç‰©ç†è¨­å‚™ (250GB è³‡æ–™ç£ç¢Ÿ)
```

**éƒ¨ç½²å¾Œé©—è­‰** (åœ¨ Phase 4 å®Œæˆå¾Œ):
```bash
# SSH åˆ° app-worker æª¢æŸ¥ LVM é…ç½®
ssh ubuntu@192.168.0.14 'sudo vgs && sudo pvs'
```

**é æœŸè¼¸å‡º**ï¼š
```bash
# vgs
  VG          #PV #LV #SN Attr   VSize    VFree
  topolvm-vg    1   0   0 wz--n- <250.00g <250.00g  â† TopoLVM VG (è‡ªå‹•å»ºç«‹)
  ubuntu-vg     1   1   0 wz--n-  <98.00g       0   â† ç³»çµ± VG
```

> **èªªæ˜**:
> - Ansible Worker Role æœƒè‡ªå‹•æª¢æŸ¥ä¸¦å»ºç«‹ LVM é…ç½®
> - å¦‚æœ VG å·²å­˜åœ¨,æœƒè‡ªå‹•è·³é (ignore_errors: true)
> - å¯é€éè¨­å®š `configure_lvm: false` åœç”¨è‡ªå‹• LVM é…ç½®

#### 1.5 TopoLVM Storage Capacity æ¨¡å¼é…ç½®

**é‡è¦**: TopoLVM ä½¿ç”¨ **Storage Capacity Tracking** æ¨¡å¼ï¼ˆKubernetes 1.21+ åŸç”ŸåŠŸèƒ½ï¼‰ï¼Œè€ŒéèˆŠçš„ Scheduler Extender æ¨¡å¼ã€‚

**é…ç½®æª”æ¡ˆ**: `argocd/apps/infrastructure/topolvm/overlays/values.yaml`

```yaml
scheduler:
  enabled: false  # ç¦ç”¨ scheduler extender (ä¸éœ€è¦)

controller:
  storageCapacityTracking:
    enabled: true  # å•Ÿç”¨ CSI Storage Capacity Tracking

webhook:
  podMutatingWebhook:
    enabled: false  # Storage Capacity æ¨¡å¼ä¸éœ€è¦ pod webhook
```

**Storage Capacity Tracking å„ªå‹¢**:
- âœ… Kubernetes åŸç”ŸåŠŸèƒ½ï¼ˆ1.21+ GAï¼‰
- âœ… ç„¡éœ€é…ç½® kube-scheduler extender
- âœ… æ›´ç°¡å–®ã€æ›´å¯é çš„èª¿åº¦æ©Ÿåˆ¶
- âœ… è‡ªå‹•å®¹é‡è¿½è¹¤å’Œå ±å‘Š

**éƒ¨ç½²å¾Œé©—è­‰** (åœ¨ Phase 4.7 å®Œæˆå¾Œ):
```bash
# æª¢æŸ¥ CSIStorageCapacity è³‡æº
kubectl get csistoragecapacity -A

# æª¢æŸ¥ TopoLVM controller æ—¥èªŒ
kubectl logs -n kube-system -l app.kubernetes.io/component=controller --tail=50
```

**é æœŸè¼¸å‡º**ï¼š
```
NAMESPACE      NAME                    STORAGECLASS           CAPACITY
kube-system    topolvm-<node>-<hash>   topolvm-provisioner    257693843456
```

#### 1.6 æª¢æŸ¥ç”Ÿæˆçš„æ–‡ä»¶

```bash
# å›åˆ° terraform ç›®éŒ„
cd /path/to/detectviz-gitops/terraform

# Ansible inventory
cat ../ansible/inventory.ini

# /etc/hosts ç‰‡æ®µ
cat ../hosts-fragment.txt
```

---

## Phase 2: ç¶²è·¯é…ç½®é©—è­‰

**ç›®æ¨™**: é©—è­‰é›™ç¶²è·¯æ¶æ§‹é…ç½®æ­£ç¢º

#### 2.1 åŸ·è¡Œç¶²è·¯é©—è­‰è…³æœ¬

```bash
cd ../scripts/
./validate-dual-network.sh
```

#### 2.2 æ‰‹å‹•é©—è­‰ï¼ˆå¯é¸ï¼‰

```bash
# æª¢æŸ¥ VM ç¶²è·¯ä»‹é¢
ssh ubuntu@192.168.0.11 'ip addr show eth0'
ssh ubuntu@192.168.0.11 'ip addr show eth1'

# æª¢æŸ¥ MTU è¨­å®š
ssh ubuntu@192.168.0.11 'ip link show eth0 | grep mtu'
ssh ubuntu@192.168.0.11 'ip link show eth1 | grep mtu'

# æ¸¬è©¦å…§éƒ¨ç¶²è·¯é€£é€šæ€§
ssh ubuntu@192.168.0.11 'ping -c 3 10.0.0.14'

# æ¸¬è©¦ DNS è§£æ
ssh ubuntu@192.168.0.11 'getent hosts master-1.detectviz.internal'
ssh ubuntu@192.168.0.11 'getent hosts master-1.cluster.internal'
```

**é æœŸçµæœ**ï¼š
- âœ… æ¯å€‹ VM æœ‰å…©å€‹ç¶²è·¯ä»‹é¢ (eth0, eth1)
- âœ… MTU éƒ½è¨­å®šç‚º 1500 (æˆ–æ‚¨è‡ªè¨‚çš„å€¼)
- âœ… å…§éƒ¨ç¶²è·¯å¯äº’é€š
- âœ… DNS æ­£ç¢ºè§£æå…©å€‹åŸŸå

---

## Phase 3: Ansible è‡ªå‹•åŒ–éƒ¨ç½²

**ç›®æ¨™**: éƒ¨ç½² Kubernetes é›†ç¾¤èˆ‡æ‰€æœ‰åŸºç¤è¨­æ–½çµ„ä»¶

#### 3.1 æª¢æŸ¥ Ansible Inventory

```bash
cd ../ansible/
cat inventory.ini

# æ¸¬è©¦ Ansible é€£æ¥
ansible all -i inventory.ini -m ping
```

#### 3.2 åŸ·è¡Œå®Œæ•´éƒ¨ç½²

```bash
ansible-playbook -i inventory.ini deploy-cluster.yml
```

**éƒ¨ç½²éšæ®µ**ï¼š
1. **[Phase 1] Common Role**: ç³»çµ±åˆå§‹åŒ–ã€å¥—ä»¶å®‰è£ã€Kubernetes å…§æ ¸åƒæ•¸é…ç½®
   - å®‰è£åŸºç¤å¥—ä»¶: `apt-transport-https`, `ca-certificates`, `curl`, `gnupg`, `python3-pip`
   - **å®‰è£ Python Kubernetes å®¢æˆ¶ç«¯**: `kubernetes`, `pyyaml`, `jsonpatch` (ä¾› ansible kubernetes.core æ¨¡çµ„ä½¿ç”¨)
   - å®‰è£ containerd (2.1.5) å’Œ Kubernetes çµ„ä»¶ (1.32.0)
   - å®‰è£ yq (YAML è™•ç†å™¨) ä¾›å¾ŒçºŒ manifest ä¿®æ”¹ä½¿ç”¨
   - é…ç½® Kubernetes å¿…è¦å…§æ ¸åƒæ•¸ï¼š
     - `net.ipv4.ip_forward=1` - å•Ÿç”¨ IP è½‰ç™¼ï¼ˆPod ç¶²è·¯è·¯ç”±ï¼‰
     - `net.bridge.bridge-nf-call-iptables=1` - æ©‹æ¥æµé‡ç¶“ iptables è™•ç†
     - `net.bridge.bridge-nf-call-ip6tables=1` - IPv6 æ©‹æ¥æµé‡è™•ç†
     - è¼‰å…¥ `br_netfilter` å…§æ ¸æ¨¡çµ„ä¸¦æŒä¹…åŒ–

2. **[Phase 2] Network Role**:
   - é…ç½®é›™ç¶²è·¯ä»‹é¢ (eth0: 192.168.0.0/24 + eth1: 10.0.0.0/24)
   - è¨­å®š /etc/hosts (detectviz.internal + cluster.internal é›™åŸŸå)
   - é…ç½®ç¶²è·¯ sysctl åƒæ•¸ (rp_filter=2 æ”¯æ´éå°ç¨±è·¯ç”±)

3. **[Phase 3] Master Role**: åˆå§‹åŒ– Kubernetes æ§åˆ¶å¹³é¢
   - åˆå§‹åŒ–ç¬¬ä¸€å€‹ master ç¯€é» (kubeadm init)
   - éƒ¨ç½² Kube-VIP (æ§åˆ¶å¹³é¢ HA çš„è™›æ“¬ IP)
   - å®‰è£ Calico CNI ç¶²è·¯æ’ä»¶
   - å…¶ä»– master ç¯€é»åŠ å…¥æ§åˆ¶å¹³é¢ (kubeadm join --control-plane)
   - **è¨­å®š kubeconfig**: ç‚º root å’Œ ansible_user (ubuntu) å»ºç«‹ ~/.kube/config

4. **[Phase 3.5] ç”Ÿæˆ Worker åŠ å…¥å‘½ä»¤**:
   - åœ¨ master-1 ä¸Šç”Ÿæˆ kubeadm join token
   - å°‡ join å‘½ä»¤å‹•æ…‹å‚³éçµ¦æ‰€æœ‰ worker ç¯€é»

5. **[Phase 4] Worker Role**: åŠ å…¥å·¥ä½œç¯€é»
   - é…ç½® LVM Volume Groups (topolvm-vg) ä¾› TopoLVM ä½¿ç”¨
   - ä½¿ç”¨ Phase 3.5 ç”Ÿæˆçš„ join å‘½ä»¤åŠ å…¥é›†ç¾¤
   - ç­‰å¾… kubelet å¥åº·æª¢æŸ¥é€šé

6. **[Phase 5] ç¯€é»æ¨™ç±¤**: ç‚ºç¯€é»æ·»åŠ å·¥ä½œè² è¼‰æ¨™ç±¤
   - master-1: `workload-monitoring=true` (Grafana, Prometheus)
   - master-2: `workload-mimir=true` (Mimir é•·æœŸæŒ‡æ¨™å„²å­˜)
   - master-3: `workload-loki=true` (Loki æ—¥èªŒèšåˆ)
   - app-worker: `workload-apps=true` (ArgoCD, æ‡‰ç”¨ç¨‹å¼)
   - **æ³¨æ„**: ä½¿ç”¨ `--kubeconfig=/etc/kubernetes/admin.conf` æ˜ç¢ºæŒ‡å®šé…ç½®æª”æ¡ˆ

7. **[Phase 6] ArgoCD éƒ¨ç½²**: å®‰è£ GitOps å¼•æ“
   - **è¨­å®šç’°å¢ƒè®Šæ•¸**: `KUBECONFIG=/etc/kubernetes/admin.conf` (ä¾› kubernetes.core.k8s æ¨¡çµ„ä½¿ç”¨)
   - å»ºç«‹ argocd namespace
   - ä¸‹è¼‰ ArgoCD å®˜æ–¹ manifest
   - ä½¿ç”¨ yq ç‚º ArgoCD çµ„ä»¶æ·»åŠ  nodeSelector (ç¢ºä¿éƒ¨ç½²åˆ° app-worker)
   - æ‡‰ç”¨ ArgoCD manifest
   - éƒ¨ç½² Root Application (App of Apps æ¨¡å¼)

8. **[Phase 7] æœ€çµ‚é©—è­‰**: é›†ç¾¤å¥åº·æª¢æŸ¥
   - ç­‰å¾…æ‰€æœ‰ç¯€é»é€²å…¥ Ready ç‹€æ…‹
   - é¡¯ç¤ºé›†ç¾¤ç¯€é»è³‡è¨Šå’Œéƒ¨ç½²æ‘˜è¦

#### 3.3 éƒ¨ç½²å¾Œé©—è­‰

```bash
# è¨­å®š kubeconfig
export KUBECONFIG=$(pwd)/kubeconfig/admin.conf

# æª¢æŸ¥ç¯€é»ç‹€æ…‹
kubectl get nodes -o wide

# æª¢æŸ¥ç¯€é»æ¨™ç±¤
kubectl get nodes --show-labels
```

**é æœŸè¼¸å‡º**ï¼š
```
NAME         STATUS   ROLES           AGE   VERSION
master-1     Ready    control-plane   10m   v1.32.0
master-2     Ready    control-plane   9m    v1.32.0
master-3     Ready    control-plane   8m    v1.32.0
app-worker   Ready    <none>          7m    v1.32.0
```

---

## Phase 4: GitOps åŸºç¤è¨­æ–½åŒæ­¥

**ç›®æ¨™**: é€é ArgoCD è‡ªå‹•éƒ¨ç½²åŸºç¤è¨­æ–½çµ„ä»¶

#### 4.1 æª¢æŸ¥ ArgoCD ç‹€æ…‹

```bash
# ç­‰å¾… ArgoCD å°±ç·’
kubectl wait --for=condition=Ready pod -l app.kubernetes.io/name=argocd-server -n argocd --timeout=300s

# æª¢æŸ¥ ArgoCD Pods
kubectl get pods -n argocd
```

#### 4.2 ç²å– ArgoCD å¯†ç¢¼

```bash
# ç²å–åˆå§‹å¯†ç¢¼
kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
echo ""
```

#### 4.3 è¨ªå• ArgoCD UI

```bash
# é¸é … 1: Port Forward
kubectl port-forward svc/argocd-server -n argocd 8080:443

# é¸é … 2: é€šé Ingress (éœ€è¦å…ˆé…ç½® DNS)
# https://argocd.detectviz.internal
```

ç™»å…¥è³‡è¨Šï¼š
- **URL**: `https://localhost:8080` æˆ– `https://argocd.detectviz.internal`
- **Username**: `admin`
- **Password**: (ä¸Šä¸€æ­¥é©Ÿç²å–çš„å¯†ç¢¼)

#### 4.4 é…ç½® Git Repository SSH èªè­‰

> **ğŸ¤– è‡ªå‹•åŒ–**: æ­¤æ­¥é©Ÿå·²åœ¨ **Ansible Phase 6** ä¸­è‡ªå‹•å®Œæˆã€‚
>
> **å‰ç½®æ¢ä»¶**: SSH ç§é‘°å­˜åœ¨æ–¼ `~/.ssh/id_ed25519_detectviz`
>
> å¦‚æœæ‚¨çš„ SSH é‡‘é‘°å·²æº–å‚™å¥½,Ansible æœƒè‡ªå‹•:
> - âœ… è¤‡è£½ SSH é‡‘é‘°åˆ° master-1
> - âœ… å»ºç«‹ ArgoCD repository secret
> - âœ… é…ç½® GitHub SSH known_hosts
> - âœ… é‡å•Ÿ repo-server ä¸¦åˆ·æ–° root application
>
##### ä»¥ä¸‹æ‰‹å‹•æ­¥é©Ÿåƒ…ä¾›åƒè€ƒå’Œæ•…éšœæ’é™¤ä½¿ç”¨

---

**æ‰‹å‹•é…ç½®æ­¥é©Ÿ** (å¦‚æœè‡ªå‹•åŒ–å¤±æ•—æˆ–éœ€è¦æ‰‹å‹•å¹²é ):

ç”±æ–¼ Root Application ä½¿ç”¨ SSH URL è¨ªå• GitHub ç§æœ‰ repository,éœ€è¦é…ç½® SSH é‡‘é‘°:

```bash
# 1. è¤‡è£½ SSH ç§é‘°åˆ° master-1
scp ~/.ssh/id_ed25519_detectviz ubuntu@192.168.0.11:/tmp/argocd-ssh-key

# 2. å»ºç«‹ ArgoCD repository secret
ssh ubuntu@192.168.0.11 "sudo kubectl --kubeconfig=/etc/kubernetes/admin.conf create secret generic detectviz-gitops-repo --from-file=sshPrivateKey=/tmp/argocd-ssh-key -n argocd"

# 3. æ·»åŠ æ¨™ç±¤è®“ ArgoCD è­˜åˆ¥ç‚º repository credential
ssh ubuntu@192.168.0.11 "sudo kubectl --kubeconfig=/etc/kubernetes/admin.conf label secret detectviz-gitops-repo argocd.argoproj.io/secret-type=repository -n argocd --overwrite"

# 4. é…ç½® repository URL
ssh ubuntu@192.168.0.11 "sudo kubectl --kubeconfig=/etc/kubernetes/admin.conf patch secret detectviz-gitops-repo -n argocd -p='{\"stringData\":{\"type\":\"git\",\"url\":\"git@github.com:detectviz/detectviz-gitops.git\"}}'"

# 5. æ·»åŠ  GitHub SSH known_hosts
ssh-keyscan github.com > /tmp/github-hostkey
scp /tmp/github-hostkey ubuntu@192.168.0.11:/tmp/
ssh ubuntu@192.168.0.11 "sudo kubectl --kubeconfig=/etc/kubernetes/admin.conf create secret generic argocd-ssh-known-hosts --from-file=ssh_known_hosts=/tmp/github-hostkey -n argocd"

# 6. é‡å•Ÿ ArgoCD repo-server è¼‰å…¥æ–°çš„èªè­‰
ssh ubuntu@192.168.0.11 "sudo kubectl --kubeconfig=/etc/kubernetes/admin.conf rollout restart deployment argocd-repo-server -n argocd"
ssh ubuntu@192.168.0.11 "sudo kubectl --kubeconfig=/etc/kubernetes/admin.conf rollout status deployment argocd-repo-server -n argocd --timeout=60s"

# 7. å¼·åˆ¶åˆ·æ–° root application
> [!NOTE]
> åœ¨åŸ·è¡Œåˆ·æ–°å‰ï¼Œè«‹å…ˆç¢ºèª `argocd/root-argocd-app.yaml` ä¸­ `spec.project` ç‚º `platform-bootstrap`ï¼Œç¢ºä¿æ ¹ Application å—æ­£ç¢º AppProject æ¬Šé™æ§ç®¡ã€‚
ssh ubuntu@192.168.0.11 "sudo kubectl --kubeconfig=/etc/kubernetes/admin.conf patch application root -n argocd -p='{\"metadata\":{\"annotations\":{\"argocd.argoproj.io/refresh\":\"hard\"}}}' --type=merge"

# 8. æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
ssh ubuntu@192.168.0.11 "rm -f /tmp/argocd-ssh-key /tmp/github-hostkey"
```

**ç­‰å¾…ç´„ 10-30 ç§’å¾Œ,é©—è­‰ Root Application ç‹€æ…‹**:
```bash
# æª¢æŸ¥ root application
kubectl get application root -n argocd
# é æœŸè¼¸å‡º: SYNC STATUS = Synced

# æª¢æŸ¥ ApplicationSets
kubectl get applicationset -n argocd
# é æœŸçœ‹åˆ°: argocd-bootstrap, detectviz-gitops
```

> [!IMPORTANT]
> `infra-appset` ä½¿ç”¨ Git Generator è¿½è¹¤ `argocd/apps/infrastructure/*`ï¼Œå› æ­¤æ¯å€‹å…ƒä»¶æ ¹ç›®éŒ„éƒ½éœ€è¦æœ‰ `kustomization.yaml` å°‡è³‡æºæŒ‡å‘ `overlays/`ã€‚è‹¥ç¼ºå°‘æ­¤å…¥å£ï¼ŒArgo CD æœƒåªçœ‹åˆ°ç©ºç›®éŒ„è€Œç„¡æ³•ç”Ÿæˆ Applicationã€‚æäº¤ä»»ä½•æ–°çš„åŸºç¤è¨­æ–½æœå‹™å‰ï¼Œè«‹åŸ·è¡Œ `kustomize build --enable-helm argocd/apps/infrastructure/<component>`ï¼Œç¢ºèªæ ¹å±¤å…¥å£ç¢ºå¯¦è¼‰å…¥ overlayï¼ˆè‹¥ç’°å¢ƒç„¡æ³•ä¸‹è¼‰ Helm chartï¼Œè«‹åœ¨è®Šæ›´ç´€éŒ„ä¸­é™„ä¸Šç­‰æ•ˆé©—è­‰ï¼‰ã€‚

#### 4.5 ç†è§£ Bootstrap åˆ†éšæ®µéƒ¨ç½²

> **ğŸ“š è©³ç´°æ–‡æª”**: `argocd/bootstrap/PHASE_DEPLOYMENT.md`

ArgoCD Bootstrap è³‡æºæ¡ç”¨**å…©éšæ®µéƒ¨ç½²ç­–ç•¥**ä¾†è§£æ±º CRD ä¾è³´å•é¡Œ:

**Phase 1: åŸºç¤è³‡æº** (Sync Wave: -10)
- âœ… ç«‹å³éƒ¨ç½²: Namespaces (cert-manager, ingress-nginx, vault, etc.)
- âœ… ä¸ä¾è³´ä»»ä½• CRDs
- âœ… ç¸½æ˜¯æˆåŠŸ

**Phase 2: é€²éšè³‡æº** (Sync Wave: 10)
- â³ å»¶å¾Œéƒ¨ç½²: Certificates, ClusterIssuers, Ingress, ArgoCDExtensions
- â³ ä¾è³´åŸºç¤è¨­æ–½ CRDs (cert-manager, ingress-nginx, argo-rollouts)
- â³ ä½¿ç”¨ `SkipDryRunOnMissingResource=true` é¿å…é æª¢æŸ¥å¤±æ•—

**é æœŸè¡Œç‚º**:
```
1. Root Application åŒæ­¥ â†’ Synced, Healthy âœ…
2. cluster-bootstrap Phase 1 éƒ¨ç½² â†’ Namespaces å»ºç«‹æˆåŠŸ âœ…
3. åŸºç¤è¨­æ–½ ApplicationSets ç”Ÿæˆ Applications (cert-manager, ingress-nginx, etc.) âœ…
4. cluster-bootstrap Phase 2 å˜—è©¦éƒ¨ç½² â†’ å¤±æ•— (CRDs å°šæœªå®‰è£) âš ï¸  é€™æ˜¯æ­£å¸¸çš„!
5. æ‰‹å‹•åŒæ­¥åŸºç¤è¨­æ–½ Applications â†’ CRDs å®‰è£ âœ…
6. cluster-bootstrap Phase 2 è‡ªå‹•é‡è©¦ â†’ æˆåŠŸ âœ…
```

**ç‚ºä»€éº¼ cluster-bootstrap æœƒé¡¯ç¤ºéŒ¯èª¤?**

åœ¨åŸºç¤è¨­æ–½åŒæ­¥ä¹‹å‰,æ‚¨æœƒçœ‹åˆ°é¡ä¼¼çš„éŒ¯èª¤è¨Šæ¯:
```
resource mapping not found for name: "argocd-server-tls"
no matches for kind "Certificate" in version "cert-manager.io/v1"
ensure CRDs are installed first
```

**é€™æ˜¯æ­£å¸¸ä¸”é æœŸçš„è¡Œç‚º**,å› ç‚º:
- Phase 2 è³‡æºéœ€è¦ cert-manager çš„ `Certificate` CRD
- cert-manager å°šæœªéƒ¨ç½²,CRD ä¸å­˜åœ¨
- ä¸€æ—¦åŸºç¤è¨­æ–½åŒæ­¥å®Œæˆ,cluster-bootstrap æœƒè‡ªå‹•é‡è©¦ä¸¦æˆåŠŸ

---

#### 4.6 é©—è­‰ ApplicationSet åŒæ­¥

```bash
# æª¢æŸ¥ Root Application ç‹€æ…‹
kubectl get application root -n argocd
# é æœŸ: Synced, Healthy

# æª¢æŸ¥ cluster-bootstrap ç‹€æ…‹
kubectl get application cluster-bootstrap -n argocd
# é æœŸ: OutOfSync, Missing (ç­‰å¾…åŸºç¤è¨­æ–½ CRDs) - é€™æ˜¯æ­£å¸¸çš„!

# æª¢æŸ¥ ApplicationSet
kubectl get applicationset -n argocd

# æª¢æŸ¥æ‰€æœ‰æ‡‰ç”¨ç‹€æ…‹
argocd app list

# æª¢æŸ¥åŸºç¤è¨­æ–½çµ„ä»¶ (éƒ¨ç½²å¾Œ)
kubectl get pods -n metallb-system
kubectl get pods -n cert-manager
kubectl get pods -n ingress-nginx
kubectl get pods -n external-secrets-system
kubectl get pods -n vault
kubectl get pods -n topolvm-system
```

#### 4.7 æ‰‹å‹•åŒæ­¥åŸºç¤è¨­æ–½ Applications

> **ğŸ“– å¿«é€Ÿåƒè€ƒ**: è©³ç´°æ­¥é©Ÿè«‹åƒè€ƒ `QUICK_START.md`

æ­¤æ™‚åŸºç¤è¨­æ–½ Applications å·²è‡ªå‹•ç”Ÿæˆ,ä½†è™•æ–¼ `Unknown` ç‹€æ…‹,éœ€è¦æ‰‹å‹•è§¸ç™¼åŒæ­¥:

**é¸é … 1: åœ¨ ArgoCD UI ä¸­æ‰‹å‹•åŒæ­¥** (æ¨è–¦)

1. è¨ªå• ArgoCD UI (https://localhost:8080)
2. é»æ“Šæ¯å€‹ `infra-*` Application
3. é»æ“Š "SYNC" æŒ‰éˆ•
4. ç­‰å¾…åŒæ­¥å®Œæˆ

**å»ºè­°åŒæ­¥é †åº**:
1. `infra-argocd` (ArgoCD è‡ªæˆ‘é…ç½® - æ‡‰ç”¨ URL è¨­å®š)
2. `infra-cert-manager` (å„ªå…ˆ - æä¾› Certificate CRDs)
3. `infra-ingress-nginx`
4. `infra-metallb`
5. `infra-external-secrets-operator`
6. `infra-vault`
7. `infra-topolvm`

> [!TIP]
> å¦‚æœ ApplicationSet æ²’æœ‰è‡ªå‹•ç”Ÿæˆä¸Šè¿° Applicationï¼Œè«‹å…ˆåœ¨ Repo ä¸­æª¢æŸ¥å°æ‡‰çš„ `argocd/apps/infrastructure/<component>/kustomization.yaml` æ˜¯å¦ä»å¼•ç”¨ `resources: - overlays`ã€‚ä¿®æ­£å¾Œé‡æ–°åŸ·è¡Œ `kubectl patch application root ...` è§¸ç™¼ `root-argocd-app` Refreshï¼Œå³å¯é‡æ–°è¼‰å…¥æœ€æ–°çš„ infra-appset é…ç½®ã€‚

**æ³¨æ„**: `infra-argocd` æ˜¯ ArgoCD çš„é…ç½®ç®¡ç†æ‡‰ç”¨,æœƒè‡ªå‹•å‡ºç¾åœ¨ ApplicationSet ä¸­ã€‚å®ƒä¸æœƒé‡æ–°éƒ¨ç½² ArgoCD æœ¬èº«,åªç®¡ç†é…ç½®æ–‡ä»¶ï¼ˆå¦‚ server URLï¼‰ã€‚

**é¸é … 2: ä½¿ç”¨å‘½ä»¤è¡ŒåŒæ­¥**

```bash
# SSH åˆ° master-1
ssh ubuntu@192.168.0.11

# åŒæ­¥æ‰€æœ‰åŸºç¤è¨­æ–½ Applications
for app in infra-argocd infra-cert-manager infra-ingress-nginx infra-metallb \
           infra-external-secrets-operator infra-vault infra-topolvm; do
  sudo kubectl --kubeconfig=/etc/kubernetes/admin.conf patch application $app -n argocd \
    -p='{"operation":{"initiatedBy":{"username":"admin"},"sync":{"prune":true}}}' \
    --type=merge
  echo "âœ… Triggered sync for $app"
  sleep 5
done
```

**é¸é … 3: ä½¿ç”¨ ArgoCD CLI**

```bash
# 1. Port forward (åœ¨å¦ä¸€å€‹çµ‚ç«¯)
kubectl --kubeconfig=/etc/kubernetes/admin.conf \
  port-forward svc/argocd-server -n argocd 8080:443 &

# 2. ç™»å…¥ (ä½¿ç”¨ Phase 4.2 ç²å–çš„å¯†ç¢¼)
argocd login localhost:8080 \
  --username admin \
  --password <your-argocd-password> \
  --insecure

# 3. åŒæ­¥æ‰€æœ‰åŸºç¤è¨­æ–½ Applications
argocd app sync infra-cert-manager
argocd app sync infra-ingress-nginx
argocd app sync infra-metallb
argocd app sync infra-external-secrets-operator
argocd app sync infra-vault
argocd app sync infra-topolvm

# 4. æª¢æŸ¥ç‹€æ…‹
argocd app list
```

**é©—è­‰åŒæ­¥å®Œæˆ**:
```bash
# ç­‰å¾…æ‰€æœ‰ Pods é‹è¡Œ
kubectl get pods -n cert-manager
kubectl get pods -n ingress-nginx
kubectl get pods -n metallb-system
kubectl get pods -n external-secrets-system
kubectl get pods -n vault
kubectl get pods -n topolvm-system

# ç¢ºèª CRDs å·²å®‰è£
kubectl get crd | grep cert-manager
# é æœŸè¼¸å‡º: certificates.cert-manager.io, clusterissuers.cert-manager.io, issuers.cert-manager.io

# æª¢æŸ¥ cluster-bootstrap ç‹€æ…‹ (æ‡‰è©²è‡ªå‹•é‡è©¦ä¸¦æˆåŠŸ)
kubectl get application cluster-bootstrap -n argocd
# é æœŸ: Synced, Healthy (Phase 2 è³‡æºå·²éƒ¨ç½²)

# é©—è­‰ TopoLVM CSIStorageCapacity è³‡æº
kubectl get csistoragecapacity -A
# é æœŸ: æ‡‰è©²çœ‹åˆ° topolvm-provisioner çš„å®¹é‡è³‡æº
```

**é æœŸçµæœ**ï¼š
- âœ… æ‰€æœ‰åŸºç¤è¨­æ–½ Applications: Synced, Healthy
- âœ… cluster-bootstrap: Synced, Healthy (Phase 2 è‡ªå‹•é‡è©¦æˆåŠŸ)
- âœ… MetalLB é‹è¡Œä¸­ (LoadBalancer æ”¯æ´)
- âœ… cert-manager é‹è¡Œä¸­ (TLS è­‰æ›¸ç®¡ç† + CRDs)
- âœ… NGINX Ingress é‹è¡Œä¸­ (Ingress æ§åˆ¶å™¨)
- âœ… External Secrets é‹è¡Œä¸­ (Secret ç®¡ç†)
- âœ… Vault é‹è¡Œä¸­ (å¯†é‘°ç®¡ç†,å¾…åˆå§‹åŒ–)
- âœ… TopoLVM é‹è¡Œä¸­ (å‹•æ…‹ PV æä¾›ï¼Œä½¿ç”¨ Storage Capacity Tracking)

**å¸¸è¦‹å•é¡Œè™•ç†**:

å¦‚æœåŸºç¤è¨­æ–½ Applications é¡¯ç¤º `OutOfSync` æˆ– `Unknown` ä½†ä¸è‡ªå‹•åŒæ­¥ï¼š

```bash
# 1. æª¢æŸ¥ ArgoCD repo-server æ—¥èªŒ
kubectl logs -n argocd -l app.kubernetes.io/name=argocd-repo-server --tail=50

# 2. å¦‚æœçœ‹åˆ°è·¯å¾‘éŒ¯èª¤ï¼Œç¢ºèª ApplicationSet é…ç½®æ­£ç¢º
kubectl get applicationset detectviz-gitops -n argocd -o yaml | grep path

# 3. æ‰‹å‹•è§¸ç™¼ root application åˆ·æ–°
kubectl patch application root -n argocd \
  -p='{"metadata":{"annotations":{"argocd.argoproj.io/refresh":"hard"}}}' \
  --type=merge

# 4. ç­‰å¾… 30 ç§’å¾Œæª¢æŸ¥ç‹€æ…‹
sleep 30 && kubectl get applications -n argocd
```

---

## Phase 5: Vault åˆå§‹åŒ–

**ç›®æ¨™**: æ‰‹å‹•åˆå§‹åŒ–ä¸¦è§£å° Hashicorp Vault

#### 5.1 ç­‰å¾… Vault Pod å°±ç·’

```bash
kubectl get pods -n vault --watch
# ç­‰å¾…æ‰€æœ‰ vault-0/1/2 éƒ½è™•æ–¼ Running ç‹€æ…‹ (0/1 Ready æ˜¯æ­£å¸¸çš„,å› ç‚ºå°šæœª unseal)
# Ctrl+C é€€å‡º watch
```

**æ³¨æ„**:
- æ‰€æœ‰ 3 å€‹ Vault pods éƒ½æœƒé€²å…¥ Running ç‹€æ…‹ï¼Œä½†é¡¯ç¤º 0/1 Ready (å› ç‚ºæœª unseal)
- å¦‚æœ vault-1/vault-2 æŒçºŒ Pendingï¼Œæª¢æŸ¥æ˜¯å¦é‡åˆ° Anti-Affinity å•é¡Œï¼ˆåƒè¦‹[å•é¡Œ #5](#å•é¡Œ-5-vault-pod-anti-affinity-èˆ‡å–®-worker-node)ï¼‰
- é…ç½®å·²ä¿®æ­£ç‚º `preferredDuringScheduling`ï¼Œå…è¨±å–® worker node ç’°å¢ƒé‹è¡Œ

#### 5.2 åˆå§‹åŒ– Vault

```bash
# åœ¨ç¬¬ä¸€å€‹ Vault Pod ä¸ŠåŸ·è¡Œåˆå§‹åŒ–
kubectl exec -n vault vault-0 -c vault -- vault operator init \
  -key-shares=5 \
  -key-threshold=3 \
  -format=json > vault-keys.json

# é¡¯ç¤ºåˆå§‹åŒ–é‡‘é‘°
cat vault-keys.json | jq
```

**é‡è¦**: å®‰å…¨ä¿å­˜ `vault-keys.json`ï¼ŒåŒ…å«ï¼š
- `unseal_keys_b64`: 5 å€‹ Unseal Keys
- `root_token`: Root Token

#### 5.3 è§£å°æ‰€æœ‰ Vault å¯¦ä¾‹

```bash
# æå– Unseal Keys
UNSEAL_KEY_1=$(cat vault-keys.json | jq -r '.unseal_keys_b64[0]')
UNSEAL_KEY_2=$(cat vault-keys.json | jq -r '.unseal_keys_b64[1]')
UNSEAL_KEY_3=$(cat vault-keys.json | jq -r '.unseal_keys_b64[2]')

# è§£å° vault-0
kubectl exec -n vault vault-0 -c vault -- vault operator unseal $UNSEAL_KEY_1
kubectl exec -n vault vault-0 -c vault -- vault operator unseal $UNSEAL_KEY_2
kubectl exec -n vault vault-0 -c vault -- vault operator unseal $UNSEAL_KEY_3

# é©—è­‰ç‹€æ…‹
kubectl exec -n vault vault-0 -- vault status

# é æœŸè¼¸å‡º:
# Sealed: false  âœ…
# Initialized: true âœ…

# è§£å° vault-1 (æœƒè‡ªå‹•åŠ å…¥ Raft cluster)
kubectl exec -n vault vault-1 -- vault operator unseal $UNSEAL_KEY_1
kubectl exec -n vault vault-1 -- vault operator unseal $UNSEAL_KEY_2
kubectl exec -n vault vault-1 -- vault operator unseal $UNSEAL_KEY_3

# è§£å° vault-2 (æœƒè‡ªå‹•åŠ å…¥ Raft cluster)
kubectl exec -n vault vault-2 -- vault operator unseal $UNSEAL_KEY_1
kubectl exec -n vault vault-2 -- vault operator unseal $UNSEAL_KEY_2
kubectl exec -n vault vault-2 -- vault operator unseal $UNSEAL_KEY_3
```

**æ³¨æ„**:
- vault-1 å’Œ vault-2 åœ¨ unseal å¾Œæœƒè‡ªå‹•åŠ å…¥ vault-0 çš„ Raft cluster
- ç¬¬ä¸‰æ¬¡ unseal å‘½ä»¤å¾Œå¯èƒ½ä»é¡¯ç¤º `Sealed: true`ï¼Œä½†æ—¥èªŒæœƒé¡¯ç¤º "vault is unsealed"
- é€™æ˜¯æ­£å¸¸è¡Œç‚ºï¼ŒVault æ­£åœ¨åŠ å…¥ Raft cluster ä¸¦åŒæ­¥ç‹€æ…‹
- ç¨ç­‰ç‰‡åˆ»å¾Œæª¢æŸ¥ pod ç‹€æ…‹ï¼Œæ‡‰è©²æœƒè®Šæˆ 1/1 Ready

#### 5.4 é©—è­‰ Vault ç‹€æ…‹

```bash
# æª¢æŸ¥æ‰€æœ‰ Vault pods ç‹€æ…‹
kubectl get pods -n vault -l app.kubernetes.io/name=vault,component=server

# æª¢æŸ¥æ‰€æœ‰ Vault å¯¦ä¾‹
kubectl exec -n vault vault-0 -- vault status
kubectl exec -n vault vault-1 -- vault status
kubectl exec -n vault vault-2 -- vault status
```

**é æœŸçµæœ**:
- æ‰€æœ‰ pods é¡¯ç¤º `1/1 Running`
- vault-0: `Sealed: false`, `HA Mode: active`
- vault-1: `Sealed: false`, `HA Mode: standby`
- vault-2: `Sealed: false`, `HA Mode: standby`
- æ‰€æœ‰å¯¦ä¾‹éƒ½åœ¨åŒä¸€å€‹ Raft cluster ä¸­ (ç›¸åŒ `Cluster ID`)

**å¦‚æœ vault pods åœ¨ unseal å¾Œä»ç„¶ 0/1**:
```bash
# æª¢æŸ¥æ—¥èªŒ
kubectl logs vault-1 -n vault --tail=50
# æ‡‰è©²çœ‹åˆ° "vault is unsealed" å’Œ "entering standby mode"

# å¼·åˆ¶åˆªé™¤ä¸¦é‡å»º pods (æœƒä¿ç•™ PVC è³‡æ–™)
kubectl delete pod vault-0 vault-1 vault-2 -n vault

# ç­‰å¾… pods é‡æ–°å‰µå»ºå¾Œå†æ¬¡ unseal
# (é‡å•Ÿå¾Œ Vault æœƒé‡æ–°é€²å…¥ sealed ç‹€æ…‹)
```

---

## Phase 5.5: Vault Kubernetes Auth é…ç½®

**ç›®æ¨™**: é…ç½® Vault Kubernetes Auth å’Œ KV Secrets Engineï¼Œç‚ºæ‡‰ç”¨éƒ¨ç½²åšæº–å‚™

**å‰ç½®æ¢ä»¶**: Phase 5 å®Œæˆï¼ŒVault å·²åˆå§‹åŒ–ä¸¦è§£å°

#### 5.5.1 å•Ÿç”¨ KV v2 Secrets Engine

```bash
# è¨­ç½® Vault Token
export VAULT_TOKEN=$(cat vault-keys.json | jq -r '.root_token')

# å•Ÿç”¨ KV v2 secrets engine
kubectl exec -n vault vault-0 -- env VAULT_TOKEN=$VAULT_TOKEN \
  vault secrets enable -version=2 -path=secret kv

# é æœŸè¼¸å‡º:
# Success! Enabled the kv secrets engine at: secret/
```

---

#### 5.5.2 å•Ÿç”¨ä¸¦é…ç½® Kubernetes Auth Method

```bash
# å•Ÿç”¨ Kubernetes auth method
kubectl exec -n vault vault-0 -- env VAULT_TOKEN=$VAULT_TOKEN \
  vault auth enable kubernetes

# é…ç½® Kubernetes auth
kubectl exec -n vault vault-0 -- env VAULT_TOKEN=$VAULT_TOKEN sh -c \
  'vault write auth/kubernetes/config \
    kubernetes_host="https://kubernetes.default.svc:443" \
    kubernetes_ca_cert=@/var/run/secrets/kubernetes.io/serviceaccount/ca.crt \
    token_reviewer_jwt=@/var/run/secrets/kubernetes.io/serviceaccount/token'

# é æœŸè¼¸å‡º:
# Success! Enabled kubernetes auth method at: kubernetes/
# Success! Data written to: auth/kubernetes/config
```

---

#### 5.5.3 å‰µå»º Vault Policy çµ¦ External Secrets Operator

```bash
# å‰µå»º policy å…è¨± ESO è®€å–æ‰€æœ‰ secrets
kubectl exec -n vault vault-0 -- env VAULT_TOKEN=$VAULT_TOKEN sh -c \
  'cat <<EOF | vault policy write external-secrets -
path "secret/data/*" {
  capabilities = ["read", "list"]
}

path "secret/metadata/*" {
  capabilities = ["read", "list"]
}
EOF'

# é æœŸè¼¸å‡º:
# Success! Uploaded policy: external-secrets
```

---

#### 5.5.4 å‰µå»º Kubernetes Auth Role

```bash
# å‰µå»º role ç¶å®š ServiceAccount å’Œ policy
kubectl exec -n vault vault-0 -- env VAULT_TOKEN=$VAULT_TOKEN \
  vault write auth/kubernetes/role/external-secrets \
    bound_service_account_names=external-secrets \
    bound_service_account_namespaces=external-secrets-system \
    policies=external-secrets \
    ttl=24h

# é æœŸè¼¸å‡º:
# Success! Data written to: auth/kubernetes/role/external-secrets
```

---

#### 5.5.5 éƒ¨ç½² ClusterSecretStore

```bash
# æ‰‹å‹•å‰µå»º ClusterSecretStore (å¦‚æœ ArgoCD åŒæ­¥å¤±æ•—)
kubectl apply -f argocd/apps/infrastructure/external-secrets-operator/overlays/cluster-secret-store.yaml

# é©—è­‰ ClusterSecretStore ç‹€æ…‹
kubectl get clustersecretstore vault-backend -o yaml | grep -A5 "status:"

# é æœŸè¼¸å‡º:
# status:
#   capabilities: ReadWrite
#   conditions:
#   - message: store validated
#     reason: Valid
#     status: "True"
#     type: Ready
```

---

**å®Œæˆ Phase 5.5 å¾Œ**:
- âœ… Vault Kubernetes Auth å·²å•Ÿç”¨ä¸¦é…ç½®
- âœ… KV v2 Secrets Engine å·²å•Ÿç”¨åœ¨ `secret/` è·¯å¾‘
- âœ… External Secrets Operator å¯ä»¥é€é Kubernetes Auth è¨ªå• Vault
- âœ… ClusterSecretStore `vault-backend` å·²å°±ç·’

**ä¸‹ä¸€æ­¥**: é€²å…¥ `app-deploy-sop.md` Phase 6.0 åˆå§‹åŒ–æ‡‰ç”¨ secrets